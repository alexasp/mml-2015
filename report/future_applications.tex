\chapter{Toward a Real-World Application}
Due to the relative novelty of differential privacy as a concept, we've found the research and discussion of the real-world applicability of a DP-based system lacking in content. We therefore want to contribute to this field by adding some of our own experiences and thoughts.  This chapter will therefore discuss the potential applications of a system based on our distributed machine learner, and some of the prerequisite criteria we've found to need fulfillment before designing an application in the first place.   

\section{Criteria for an application} \label{sec:suitability_criteria}
What we've discovered during our pre-studies and experimentation have resulted in the following suitability criteria for a potential application. 
\begin{enumerate}
	\item The interest in privacy is strong, and the risk of re-identification is significant.
	\item The information to be analyzed must be stored in some form of a structured database.
	\item The amount of data must be significant.
	\item The analysis of the data can tolerate some distortion in the information from the database.
	\item The analysis of the data do not involve study of outliers, or other individuals.
\end{enumerate}
These criteria are not meant to provide an all-compassing set of rules that when followed will mean instant success for a given application, but rather as a definition of characteristics that can lead to good application opportunities if considered.  

\subsection{Strong privacy interest}
As a first criterion, a solid interest in privacy is necessary due to implementation of a differential privacy mechanism will lead to a trade-off between utility and privacy. As we've have shown in our analysis the effects of this trade-off can be minimized, but stronger privacy guarantees will inevitably lead to decaying accuracy due to the noise addition. The underlying assumption in this criterion is therefore that the risk of re-identification is significant, but manageable to secure by applying a differential privacy mechanism. The potential for future re-identification should be a factor in determining the strength of the privacy risk, but it should not prevent consideration of other factors, such as the potential public utility one can gain from data analysis.

\subsection{Structured Data}
The mechanisms that were originally designed to support differential privacy were created to address the threats to privacy from the sharing of information contained in a centralized database. We've shown through our experimentation that there are an potential applicability in designing a distributed machine learning system, but there are still a criterion that the data must be structured and labeled before any form for learning can be applied. Future experimentation can potentially prove that this criterion can be relaxed. This can then open avenues for exploring the usage of other branches of machine learning, such as deep learning and unsupervised learning. For the moment however, structured data is an important prerequisite.

\subsection{Data size}
As we showed in Section \ref{importance_of_data}, the amount of data available for learning can have massive impact on the accuracy of the resulting classifier. This is a direct result of equation \ref{eq:aggregated_logistic_sensitivity}, which specifies the upper bound on the sensitivity for distributed logistic regression, i.e the noise needed to hide a record in the dataset. Since a higher value of $n_j$, i.e a larger amount of data in the smallest dataset, will lead to a smaller amount of noise added to each aggregated model and generally better accuracy. The exact amount of data needed to create a decent classifier is difficult to exactly quantify, but our results indicate that a smaller number of records than that initially tested by \cite{pathak2010diffprivhomo} is truly needed to construct a decent classifier. The value of the privacy parameter $\epsilon$ will also play a factor in the amount of data needed, as a stronger guarantee of privacy requires a larger amount of data (see Figure \todo{put in correct ref}).

\subsection{Tolerance for data distortion}
The fourth criterion is that the usage of the data analysis must tolerate some distortion. Since our mechanism for supporting differential privacy function by inserting Laplacian noise, the resulting classifier will be a distorted version of the "true" classifier. The users of a system based on our framework must therefore be willing to accept a potential loss of utility due to distortion in return for a privacy guarantee for their data. 

Certain studies of large datasets might not be able to accommodate even small drop in accuracy in their system. Examples of this would be studies of health care data from hospitals or precise scientific research, where just a small source of error might render the results inoperable. Another example is the study of census data, where a researcher noted that the introduction of noise resulted in demographic research errors \citep{yakowitz2011tragedy}. 

This is not to say that research on health data or census data is completely infeasible, as both our own research on the Adult dataset (US Census data) and the research of \cite{ji2014DisLogReg} on health data has shown promising results. What it means is that certain considerations need to be taken when performing these studies, such as simplifying the problem, find workarounds for dealing with wider margin of errors, etc. 

\subsection{Study type}
The fifth criterion for adopting differential privacy is that it precludes the study of outliers. The goal of such a study is inconsistent with the differential privacy goal of preventing the identification of the presence or absence of a record in a database. An application based on our architecture should therefore instead focus on finding similarities between users, and how to best leverage the aggregation mechanism we employ. 


\section{Potential future applications}
\subsection{Wearable health sensor data analysis }
A growing worldwide market is the sale and usage of wearable sensors, such as environmental sensors, motion sensors, and health sensors. A IHS report \citep{ihs2014reportwearables} from 2014 estimates that the market for sensors in wearables will expand to 135 million units in 2019, up from 50 million in 2013. These wearables will evolve from being just a single purpose device such as a pedometer and grow into more multipurpose devices such as a smartwatch, which will consist of several sensors which can monitor several components within its area of use. 

The wearable devices are implementing fitness and health monitoring by using a mixture of sensors, such as motion, pulse, hydration and skin temperature sensors. All of these wearables will therefore generate a massive amount of data about the person who are using them. This data can be considered as highly sensitive information, as it can unveil a lot about their user's health, and the manufacturers of these devices knows this. Dana Liebelson, a reporter for Huffington Post, queried several US-based fitness device companies about their privacy. One of the replies she got, was that "the company does not sell information collected from the device that can identify individual users", but that they were considering marketing aggregate information that cannot be linked back to an individual\citep{liebelson2014wearables}. As we saw in section \ref{section:privacy_breaches} and \ref{section:attack_vectors}, many of the popular methods for aggregating and anonymizing a dataset carries an inherent risk of a privacy breach.

A study \citep{raij2011privacyConcernWearables} have been performed to try to measure the privacy concerns of people using wearable sensors, and found that activity trackers that monitor heart rate, steps, and pulse for instance, was usually seen as inoffensive to the usersâ€™ privacy at the start of the study. The researchers then had some of the testers wear sensors, and could from the resulting data infer periods of heightened stress, as well as derive certain context and behaviors that could trigger the aforementioned stress. The participants were then given a similar questionnaire to the initial one, and many then reported a heightened sense of concern. 
  
This is where we see a potential application for our distributed framework. Although there would be some initial problems due to our learner at the moment requiring labeled data to create a classifier, there can be potential in an health application where users keep control of their own data. According to IBM research, areas in which enhanced data and analytics yield the greatest results include: pinpointing patients who are the greatest consumers of health resources or at the greatest risk for adverse outcomes; providing these individuals with the information they need to make informed decisions and more effectively manage their own health as well as more easily adopt and track healthier behaviors\citep{IBM2013reportBigDataHealth}. While pinpointing a single user might not be feasible due to the differential privacy guarantee, there could be merit in an application where the user could be anonymously classified if he or she have a potential health risk such as diabetes. This classification would be based on the model created by their local data. The user could then be sent information on how to handle their health more efficiently, and potentially also be given an exercise regime that could be monitored through their wearables. 

Evaluating this application against the suitability criteria we suggested in Section \ref{sec:suitability_criteria}, we can see that a health application would have little trouble fulfilling the first three criteria. While the public attitude towards their privacy can vary from the completely unconcerned to a small proportion of the public that has strong views on privacy, research would suggest that people have increasing expectations on security and choice about access to their records \citep{singleton2008public}. The two next criteria that deal with data structure and size should be relatively easy to fulfill, as a wearable health sensor would quickly register a plethora of data records for use in analysis. Looking at the estimates for market expansion there is definitely potential to gain a critical user mass due to the increasing interest in such devices.

The two next criteria will most likely prove a bigger challenge to overcome, as earlier research on privacy-preserving health data analysis have found that a high-dimensional data generally makes the usage of differential privacy inappropriate due to degrading utility \citep{gardner2013share,mohammed2013privacy}. One would therefore need to be careful when deciding the goal of study, and start out with a well-thought out plan of data collection, analysis, and visualization.  

%data in healthcare is overwhelming not only because of its volume but also because of the diversity of data types and the speed at which it must be managed \citep{raghupathi2014big}. 
%
%-Future homes can potentially track you through you phone, or similar device. 
%
%-You don't want to send this data to someone else, but what you can learn from the data can be highly useful in your daily life if analyzed. 
%
%Kevin Fong and the England rugby team. Monitor heart rate, step balance, and a lot of other factors. Can pick up injuries and illnesses well before any doctor. This will trickle down into daily use over the next decade, and let people potentially discover illnesses before they even occur. 

\subsection{Private sharing of business data}
A potentially interesting and lucrative market can be found in facilitating the sharing of data between businesses in a private manner. Our motivating example is found in the business of oil market analysis, where competing firms gather a lot of data about oil price, rig placements, supply ship availability, and more. They use this data to create analytical models which help them in their work, and they also sell this information to external clients. Often the firms would like to collaborate their models or their data with their competitors. This could be to validate that they are seeing the same trends or any other reason, but due to the sensitive nature of their data and their fear of losing a competitive edge, they cannot do this in a practical way. 

It is in a situation like this that our distributed learner could be applied, and allow the sharing of data between competitors as our system could provide a privacy guarantee to all of the participants. The participants would never lose control of their data, as all they would need to install a program that allows them to connect as a peer in our network: No third party would ever need access to their data. Since the goal of the application is to privately collaborate and find common trends in data from different parties, this would fit perfectly in with the 4th and 5th suitability criteria. 

The challenge would come mainly from trying to employ a common data structure, as competing firms will most likely have their own way of handling the data they collect. An application would need to be built with this in mind, as data would need either a common format or a good amount of pre-processing. The latter could be solved by providing an add-on feature to the application that would allow the businesses to easily pre-process the data on their own schedule before making them available for analysis.    



\cleardoublepage 