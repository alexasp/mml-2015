%===================================== CHAP 5 =================================

\chapter{Reflections and Conclusion}

\section{Reflections on Privacy and Utility } \label{sec:privacy_utility_reflection}

Although the promise of differential privacy (see Section \ref{section:differential_privacy}) seems to be an ideal guarantee for each of the data subjects in a data set, it has received mixed reviews from legal scholars and computer scientist on its usefulness for resolving the privacy-utility trade-off. Narayanan and Shmatikov praised differential privacy as "a major step in the right direction\citep{narayanan2010myths}". Sarathy and Muralidhar on the other hand, contend that differentially private mechanisms are impracticable in computationally intensive context, as well offering either very little privacy or very little utility\citep{Sarathy2011evaluating}. Xiao et al. also criticized that the mechanisms often placed undue burdens on data researchers due to decreased utility, especially with large data sets used in research on populations\citep{xiao2011differential}. UCLA law professor Paul Ohm presented a legal review of current anonymization techniques, including differential privacy, and practically dismissed it as a solution from a legal standpoint. His critique was based on several factors; Such as the mechanism not being very intuitive, having limited usefulness in high noise situations, and potentially bug-prone security functions. He concluded that "utility and privacy are, at bottom, two goals at war with one another. In order to be useful, anonymized data must be imperfectly anonymous\citep{ohm2010brokenPrivacyPromise}."

All of the aforementioned critique of differential privacy can be considered valid criticism in certain situations, as we have experienced the difficulty of balancing the utility-privacy trade-off ourselves during our project. Our experiments have shown that there are several parameters outside of just the privacy parameter $\epsilon$ that need to be fine-tuned, such as regularization, learning rate, and group size. Small changes in the composition of these three can lead to wildly varying results for our classifier, but at least these parameters can be tuned only with the intention of maximizing the utility of the resulting classifier. When tuning the $\epsilon$ parameter on the other hand, one always have to keep in mind the trade-off. This tuning is made harder due to the exponential nature of the privacy guarantee, as well as its not well-defined bounds. As Dwork herself has stated\citep{dwork2008differential}: 
\begin{quote}
	The choice of $\epsilon$ is essentially a social question [...]That said, we tend to think of $\epsilon$ as, say, 0.01, 0.1, or in some cases, ln2 or ln3. If the probability that some bad event will occur is very small, it might be tolerable to increase it by such factors as 2 or 3, while if the probability is already felt to be close to unacceptable,then an increase by a factor of $e^{0.01} \approx 1.01$ might be tolerable, 	while an increase of $e$, or even only $e^{0.1}$, would be intolerable.
\end{quote}

Our own experiments have indicated that the ideal range for $\epsilon$ lies between 0.1 and 1.0, which fits reasonably well with Dwork's description. At lower values than 0.1, the error rate quickly rise as well as the error variance between the peers, meaning that an $\epsilon$ value of 0.01 returns too much noise to be of any utility. On the Spambase data set for example, the error rate increased from 0.168 to 0.336 when $\epsilon$ changed from 0.1 to 0.01. A similar change from 1.0 to 0.1 however, only decreased the result by one percentage point, meaning the error rate went from 0.158 to 0.168.     

We will therefore conclude that although the trade-off between privacy and utility is by no means solved by our current efforts, we have shown that it is not such a contradictory problem as some people might claim. By performing careful pre-processing and extensive parameter tuning, one can achieve useful results from classification even though the models are obscured by differential privacy.   


\section{Reflection on the practical applicability of differential privacy}
As we can tell from the previous section, differential privacy has been met with both enthusiasm and criticism. In this section we will reflect on some of the strongest criticism of differential privacy, such as what was presented in the paper by \cite{Sarathy2011evaluating}, as well as \cite{ohm2010brokenPrivacyPromise}. Both of these papers try to discredit the applicability of differential privacy by presenting mostly legal arguments, mainly targeting their criticism on a single mechanism: the output perturbation. These authors seems to be assume that this is the only way of providing differential privacy, which would only be true if all the literature on differential privacy consisted only of Dwork's original paper from 2006. 

In the paper by \cite{Sarathy2011evaluating}, the opening example is of an internist at a hospital, which query the database with differential privacy protection on the output. They then claim: "even knowing the distribution of noise that is randomly added to each cell, the internist has no hope of interpreting the response. The true values could be almost anything." While this example is technically correct, it is flawed by the fact that no such system would be allowed to exist in the first place, especially only with the basic output perturbation mechanism. As we pointed out in Chapter \ref{ch:RW_application}, a system need to consider certain criteria before development, and design it accordingly. In a healthcare based information system, certain considerations would certainly be necessary; One would definitely need a lax privacy guarantee, and data would need to be aggregated in a manner that would decrease the sensitivity as much as possible to require the least amount of noise possible to hide.

That is not to say that there are not valid criticism in Sarathy's paper: differential privacy can only make probabilistic guarantees, and if your legal standard is stricter than that, it might not be the best choice. Their rhetoric on the other hand, with claims such as: "differential privacy is either not practicable or not novel" seems to be unfounded at best when you consider their representation of differential privacy is basically a straw-man argument. If legal scholars such as Sarathy and Ohm intends to level criticism from a legal standpoint, it would be advisable to form an objective criticism after considering the entirety of the research field, instead of  selectively picking one aspect.

In their effort to discredit differential privacy, the authors have ignored both how academic and business research work to address these problems that they have raised. Research are being still being performed on new differential privacy mechanisms with optimal utility \citep{eigner2014privada}, and there has also been research on creating an economic model for choosing the epsilon value when designing a data study \citep{hsu2014economicEpsilon}. On the business side, Google have released a framework called RAPPOR, based on a concept called randomized response \citep{erlingsson2014rappor}. Additionally, a study by \cite{chin2012differential} infers that Facebook appears to be using differential privacy-supporting technologies in its targeted advertising system without apparent loss of utility.

Based on these developments, as well as our own findings in this area, we find it safe to conclude that differential privacy has great potential for real-world applicability. Despite of all the critique and the difficulty it brings, we have still found it entirely rewarding to perform research on the usage of differential privacy. As a solution, it is not an optimal fix-all concept which can be applied to every form of machine learning problem and provide privacy for the participants. There is still research to be done on how to make it work on various forms of data, and also on how to make it in an online setting. All of these steps will hopefully lead to a solution that can one day work in a way so that people will have control of their own data, make it available for research and still be guaranteed that their data and their privacy is safe. 

\section{Reflection on implementation challenges and solution}
While JADE was useful in offering an intuitive abstraction for modelling peers, it was challenging to use dynamically. In particular, resetting experiments proved difficult. Since creating a JADE environment creates a new process which also opens a particular TCP port. If a JADE environment was created using the same port, the new instance would fail to start up. Since creation and teardown of the JADE platform is not instantaneous, it was necessary. We also had issues with re-registering new instances of some of the agents we used, since the unregistering is an asynchronous call. All in all, it might have been simpler and less error prone to create a multi-threaded application using a tool that required less investment, like OpenMP. On the other hand, such an implementation would not lend itself as easily to a production prototype - our JADE implementation could with some extensions be tested on in a setting with many devices.

\section{Conclusion and final remarks}
\textbf{RQ1: How big is the loss of accuracy in a distributed, differentially private system, compared to a centrally trained model?} \\
We have found that under ideal conditions and perfectly tuned parameters, the loss of accuracy can be minimized to a negligible difference compared to a centralized and noise-less solution. We have however noted in our reflection that the research community have not yet reached consensus on appropriate standards for the privacy parameter $\epsilon$, but our results indicate that a range between 0.1 and 1.0 seems to yield the best results. Further testing and standardization by the community is ultimately needed before deciding in which situations different values will be appropriate.

\vspace{2mm}
\noindent
\textbf{RQ2: How can the variance in accuracy between participants be minimized?} \\
Since our tests involved uniform distributions of data among the participants, we observed very low variance in accuracy even in the predictions of locally trained models. While it is clear that variance will be zero if all peers use the same, aggregated models, the gain is very small. Due to this, we have to conclude that in a setting with uniform data distribution among participants, there is no compelling reason to use the aggregates if the goal is to reduce variance. We believe that experiments in future work conducted with non-uniform data distribution might lead to a different conclusion.

Note that we did in some cases see better error rate for the aggregated model than the locals, so when considering both the error rate and its variance the aggregated model can be the best choice despite the insignificant difference in variance.

\vspace{2mm}
\noindent
\textbf{RQ3: Can we validate and enhance earlier research in distributed differentially private machine learning?} \\
We have validated the research performed by \cite{pathak2010diffprivhomo} by using their proposed solution as a baseline, and then extending it with some design of our own. We've ran similar tests on the same dataset as they did, and found that our framework classified with a mean error of 0.165 compared to their best case of 0.24. When we ran the experiment with the same configuration as them, ergo without our upgrades, we achieved a mean error of 0.185. This indicates that they might have an non-optimal implementation, or that some details of their configuration might be missing or misleading. 

\section{Threats To Validity}
\todo[inline]{This section with threats to validity should be moved somewhere else. It does not fit in a reflections/conclusion chapter. Maybe at the end of the analysis? }

\subsection{Platform}
A potential threat to the validity of our conclusion/work \todo{Fix this sentence}, is how we performed the setup of the Jade platform. Since we wanted to perform our experiments on over a range of parameters, we needed to find a way to reset the platform after a successive experiment and re-run it with a new set of configurations. We solved this by having a jade agent called CompletionAgent be responsible for waiting for every peer to message indicating their completion, which would trigger the CompletionAgent to deregister all the peers from the MainContainer and then reset the whole environment. The environment would then be set up again with new parameters. 

What we see as a potential source for concern in this process is the possibility for error during the deregistration. During the implementation of this process we encountered some problems in making it work, as the CompletionAgent seemed to take an unreasonable amount of time in completing its purpose. Although we found a solution to this problems, there is still a risk that peers do not deregister as they should and carry through into the next iteration of testing. This could lead to false information being injected into our experiment, which would skew our results.  

We have however minimized this risk by continuously developing unit test to verify new code additions, as well as using JADE's native GUI to supervise the behavior of the peers while running. We therefore conclude that the risk is negligible.

\subsection{Scalability and Resource Consumption}
%A clear weakness of our system is our lack of formal analysis of scalability and resource consumption. In our problem statement we stated that we wanted to create an framework that allows for distributed and scalable machine learning, but we lacked both the time and the background knowledge to perform such an analysis. Due to time constraints, we've had to take certain shortcuts while implementing our systems. The most glaring liability for system scalability is our use of a single GroupFormingManager to handle allocating aggregation groups for the participating peers. This manager would quickly become a bottleneck in our system if we wanted to scale the amount of peers beyond just a small mass of users. Our solution to this predicament is found in Section \ref{sec:Future Work}, where we provide a solution in the form of the Newscast algorithm. 

\subsection{Homomorphic encryption}
As mentioned in Section \ref{sec:homomorphic_encryption}, homomorphic encryption is still in an infantile stage of development and therefore cannot be called a well-proven technology. Our method for aggregating models from various peers is based on a homomorphic encryption scheme developed by Pathak et al. but due to time constraints we could not actually implement it and instead had to opt for simulating the results of applying this scheme. We therefore do not have real-world results that can validate the applicability of this scheme, nor do we know if applying this scheme would lead to increased run-time and resource consumption. While this remains an interesting area for future research, as it stands now it remains a possible threat to the validity of the results we've achieved and therefore also the conclusion we have drawn from them.  

\subsection{Validation Scheme}
As we mentioned in Section \ref{sec:cross_validation}, we chose not to implement stratified cross validation as recommended by \cite{kohavi1995crossvalidation}. At the time the decision was made, the data sets we employed had close to uniform class distribution, which would most likely not gain any form of improvement from employing stratification. As the experiment progressed, we added the Adult data set to our experimental procedure, which has a class distribution of ca 75/25\%. Instead of re-doing all of our previous experiments, we instead chose to keep on validating with the normal form of cross-validation. 

This choice might have introduced a small source of error into our results from the Adult data set, as \cite{kohavi1995crossvalidation} notes that stratification is generally better scheme in these situations, both in terms of bias and variance. We defend our choice by pointing out that Kohavi's paper report that the biggest improvement in stratification comes when using a low amount of folds, and only shows minuscule improvement in the case of 10-fold which is what we employ. The advantage of stratification is also most apparent in data sets with many categories, whereas all our data sets are binary. We therefore conclude that the variance of the results on the Adult data set might be slightly worse than the optimal solution, but the risk of this is so negligible compared to other sources of variance, such as the noise and regularization parameters.     

\section{Future Work} \label{sec:Future Work}
In this section we propose some future areas of research which could lead to significant improvements for our framework. The three first suggestions are topics we really wish we could have achieved during this thesis project, but there just wasn't enough time and resources available. The last three suggestions are much more comprehensive, and can possibly be considered a whole new project or research area.  

\vspace{2mm}
\noindent
\textbf{Further develop and test the propagation of aggregated models}: We experienced that when we shared the aggregated models globally in our network, we could decrease the SD in our classification error, as well as sometimes improving the classifier. Further research should go in expanding this behavior, as you could potentially propagate models only to peers in geographic and/or demographic vicinity. This could possibly lead to more specialized models, which could give better classification rate to a specialized subset of peers. 

\vspace{2mm}
\noindent
\textbf{Test on datasets with uneven distribution}: Another important area of research would be to further test the applicability of peers sharing data to create better aggregated models. Our original research questions was designed to explore the validity of our proposed method of doing differentially private machine learning, and our current research has been limited to testing on a small amount of data sets which is publicly available. In the future more research is needed on data sets with an uneven underlying distribution, which could potentially provide results highlighting the usefulness of sharing information between peers. An ideal data st would be one where each peer only holds data which makes up only a part of the solution. 

\vspace{2mm}
\noindent
\textbf{Implement the Newscast algorithm for selecting peers}: The Newscast algorithm is a gossip protocol which facilitates a robust spread of information. The core of the protocol involves periodic and pairvise interaction between processes. Implementing this algorithm would allow our system to scale better when a big number of peers are added to the network. The biggest bottleneck of our system at the moment is the peer sampling during the group forming, as it requires a single agent to act as a manager for how groups are formed. The basic idea of the Newscast algorithm is that each node, or peer in our situation, has a partial view of the system. All nodes exchange their views periodically, which allows them to keep an up-to-date view locally and spread their information throughout the network. Further research into this algorithm would allow us to customize this algorithm so that peers in our network could form groups based on their partial views of the network. 

\vspace{2mm}
\noindent
\textbf{Full data protection for each peer's data}: This would involve dividing the epsilon by the biggest data set size, as formalized by \cite{dwork2013algorithmic}. This is an ever tighter privacy guarantee, but it would potentially mean that the results would contain too much noise. To test this we would need a massive data set, as we would need to test the correlation between data set size, and amount of noise added to each peer. The reason for this is that more noise needs more data to smooth out. This guarantee would also need to be formalized mathematically, as full data protection is still a very new and a underresearched area.   

%Real world case which takes humans into account. Right now research in privacy is all about the technical details, and try to get it as close as possible to existing methods. Without some kind of popular support, the method will never see practice in real-world applications. 
\vspace{2mm}
\noindent
\textbf{System that works in an online setting}: It could potentially improve the system, as you would have new data coming in which could replace old data with spent budgets, but it would also be potentially a big trade-off as you won't have the same data history as you would have in a system without differential privacy. Dwork has written about this in her book, so we can take inspiration from there. 

\vspace{2mm}
\noindent
\textbf{Security mechanisms for stopping sabotage}: In our current system we have assumed that the peers will be honest-but-curious when sharing their data, meaning that we have no way of detecting dishonest peers. In a real world system there would need to be safeguards against people which intend to either destroy the validity of the classifier created by feeding misinformation into the system, or people who tries to intercept and expose the data from other peers. Potential research ares would be intrusion detection in distributed systems, fraud detection, trust networks and reputation systems, and further research into encryption.  



\newpage
\listoftodos[Notes]

\cleardoublepage