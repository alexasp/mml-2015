% !TeX spellcheck = en_US
%===================================== CHAP 3 =================================

\chapter{Basic Theory}
In a world where massive amounts of sensitive personal data are being collected, attacks on the individual's privacy are becoming more and more of a threat. One type of attack is the identification of an individual's personal information from massive data sets, such as people's movie ratings from the Netflix data set\cite{narayanan2008robust}, and the medical records of a former governor of Massachusetts\cite{barth2012re}. These types of privacy breaches may lead to the unwanted discovery of a person's embarrassing information, and could also lead to the theft of an individual's private data or identity.  

Many different approaches have been tried by data custodians to privatize the data they hold, such as removing any columns containing Personally Identifiable Information (PII), anonymizing the data by providing k-anonymity protection\cite{sweeney2002k}, or perform group based anonymization through l-diversity\cite{machanavajjhala2007diversity}. All of these methods mentioned have been proved to be susceptible in some way or form to attacks \cite{ganta2008composition}. Motivated by these shortcomings, a researcher at Microsoft came up with a data theoretical framework called differential privacy, which operates off a solid mathematical foundation and have strong theoretical guarantees on the privacy and utility of the released data.

\section{Differential Privacy}
The term "differential privacy" was defined by Dwork as a description of a promise, made by a data holder to a data subject: "“You will not be affected, adversely or otherwise, by allowing your data to be used in any study or analysis, no matter what other studies, data sets, or information sources, are available." \cite{dwork2013algorithmic}
In an ideal situation, databases which implement differential privacy mechanisms can make confidential data widely available for accurate data analysis, without resorting to data usage agreements, data protection plans, or restricted views. Nevertheless, the Fundamental Law of Information Recovery states that overly accurate answers to too many questions will destroy privacy in a spectacular way \cite{dwork2013algorithmic}, meaning that data utility will eventually be consumed.

\subsection{Definition of Differential Privacy}
The classic example for explaining a security breach is the case of Mr White: Suppose you have access to a database that allows you to compute the income of all residents in a specified area. If you knew that Mr White was going to move, simply querying the database before and after his relocation would allow you to deduce his income. 

\textbf{Definition 1}: a mechanism $\tilde{f}$ is a random function that takes a dataset D as input, and outputs a random variable $\tilde{f}$(D).

\textbf{Definition 2}: the distance of two datasets, d(D, D′), denotes the minimum number of sample changes that are required to change D into D′.

Formally, differential privacy is defined as follows:
A randomized function $f$ gives $\epsilon$-differential privacy if for all data sets D and D' differing on at most one row, and all S$\subseteq$Range($\tilde{f}$),

 $$Pr[\tilde{f}(D)\in S]\leq \mathrm{e}^{(\epsilon)}\times P[\tilde{f}(D')\in S]$$
 
What this means is that the risk to an individual's privacy should not be substantially increase as a result of participating in a statistical database, as the risk is bounded by the parameter $\epsilon$. Therefore an attacker should not be able to learn anything about any participant that they would not have learned if the participant had opted out of participating.


\subsection{Noise Mechanisms}
This is guaranteed by applying noise to the result of an query to the dataset, by using the function $f()$. \\
\textbf{Definition 3}: A query $f$ is a function that takes a dataset as input. The answer to the query $f$ is denoted $f(D)$.

There are many different mechanisms for applying this noise, but the two most common are the Laplace mechanism and the Exponential mechanism. 

\subsubsection{Laplace Mechanism}
The Laplace mechanism involves adding random noise which follows the Laplace statistical distribution. The most common question that needs to be answered before doing research with differentially private data, is how we should define our Laplace random variable, i.e how much noise needs to be added. The Laplace distribution centered around zero has only one parameter, its scale, and this is proportional to its standard deviation. The scale is naturally dependent on the privacy parameter $\epsilon$, and also on the risk of the most different individual having their private information leaked from the data. This risk is called the sensitivity of the query, and is defined mathematically as:

$$\Delta \mathit{f}=\underset{D,D'}{max}||\mathit{f(D)}-\mathit{f(D')}||_{1}$$

This equation states that the maximum difference in the values that the query \textit{f} may take is on a pair of databases that differ on only one row. Dwork proved that adding a random Laplace variable, $(\Delta f/\epsilon)$, to a query you could guarantee $\epsilon$-differential privacy\cite{dwork2013algorithmic}. 

\subsubsection{Exponential Mechanism}
The Exponential mechanism is a method for selecting one element from a set, and is commonly used if a non-numeric valued query is used. An example would be: "What is the most common eye color in this room?". Here it would not make sense to perturb the answer by adding noise drawn from the Laplace distribution. Instead we would use the aforementioned mechanism and make it exponentially more likely to make high quality outputs. 

\textbf{Definition 4}: the sensitivity of score function H is defined as
$$s(H, ||.||) = \underset{(D,D′)=1,a\in A}{max}||H(D, a)-H(D',a)||$$
The exponential mechanism: given a dataset D and a set of possible answers
A, if a random mechanism selects an answer based on the following probability,
then the mechanism is ǫ-differentially private:
P(a ∈ A is selected) ∝ e
ǫH(D,a)/2s(H,k.k) 
\cleardoublepage
