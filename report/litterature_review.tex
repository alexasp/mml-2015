%===================================== CHAP 2 =================================

\chapter{Background and Motivation}

In this section we will first explain some basic concepts and expressions that are used in the privacy context such anonymization operations and Personally Identifiable Information(PII). Then we will have a look at some classic examples of failure to preserve privacy when data publishing and how these attacks motivated us to choose our topic for this thesis.

\section{Concepts and Expressions}
In the most basic form of privacy-preserving data publishing (PPDP), the data holder has a table of attributes from the following four categories: Explicit Identifier, Quasi Identifier, Sensitive Attributes, and Non-Sensitive Attributes \cite{fung2010privacybook}. A summary of each category can be found in Table \ref{table:DataAttributesTable}.
\begin{table}[H]
	\begin{tabulary}{\textwidth}{|L|L|L|}
		\hline \textbf{Attribute name}  & \textbf{Definition} & \textbf{Example} \\ 
		\hline   Explicit Identifier &  Explicitly identifies record owners & Government identity number (e.g SSN)  \\ 
		\hline   Quasi Identifier(QID)  & Potentially identifies record owners & Birth date and gender  \\ 
		\hline   Sensitive Attributes & Sensitive information about a person & Income, disability status  \\ 
		\hline   Non-Sensitive Attributes & All other attributes & Favorite band  \\ 
		\hline 
	\end{tabulary} 
	\caption{Table of basic categories of database attributes}
	\label{table:DataAttributesTable}
\end{table}
From these categories, it would be easy to think that Personally Identifiable Information (PII) would only be found in the first attribute. As we will see in the next section, this is not the case. Recent privacy laws have defined PII in a much broader way. They account for the possibility of deductive disclosure and do not lay down a list of attributes that constitutes as PII. For example, the European Parliament made a set of directives known as the Data Protection Directive, in which personal data is defined as:  “any information relating to an […] natural person […] who can be identified, directly or indirectly, in particular by reference […] to one or more factors specific to his physical, physiological, mental, economic, cultural, or social identity”\cite{EUdataprotection2006}. 

In order to remove any PII from a dataset, it needs to go through a process called anonymization. This constitutes a series of modifications/manipulations of with the ultimate end goal of protecting the privacy of the dataset's participants. Fung et al.\cite{fung2010privacybook} operates with a number of five basic operations which might be applied for this purpose. These operations are shortly described in Table \ref{table:AnonymizationTable}.
\begin{table}[H]	
		\begin{tabulary}{\textwidth}{|L|L|}
			\hline \textbf{Anonymization Operation} & \textbf{Definition }\\ 
			\hline Generalization & Replaces the value with more general value, such as a mean value \\ 
			\hline Suppression & Replaces the value with a special value, indicating that the replaced values are not disclosed \\ 
			\hline Anatomization & De-associates the relationship between the quasi-identifier and sensitive information  \\ 
			\hline Permutation & Partitions a set of data records into groups and shuffles their sensitive values    \\ 
			\hline Perturbation & Replace the original value with a synthetic value that keep the statistical characteristics  \\ 
			\hline 
		\end{tabulary} 
		\caption{Table of anonymization operations (adapted from \cite{fung2010privacybook} )}
		\label{table:AnonymizationTable}
\end{table}
\section{Privacy Breaches}
In the recent years there have been many failures in privacy preserving data publishing. Many companies have been faced with a PR disaster after releasing data about their customers thinking them being anonymized, only to have people de-anonymize their data and breaching the privacy of the datasets' participants. In this section we will have a look at some of these privacy failures.

\subsection{Netflix Prize Competition}
Netflix, the world's largest online movie streaming website, decided in 2006 to crowdsource a new movie suggestion algorithm and offered a cash prize of 1 million dollar for the most efficient algorithm. To help the research, they released 100 million supposedly anonymized movie ratings from their own database. In order to protect the privacy of their users, Netflix removed all user level information; such as name, username, age, geographic location, browser used, etc. They also deliberately perturbed "some of the rating data for some customers[...] in one or more of the following ways: deleting ratings; inserting alternative ratings and dates, and modifying random dates"\cite{bell2007netflix}. The released data records included an anonymized user ID, movie name, date of rating, and the user's rate on a scale from 1 to 5. 

Two researchers from the University of Texas,Narayanan and Shmatikov\cite{narayanan2008robust}, demonstrated that an adversary who knows only a little bit about an individual subscriber can easily identify this subscriber's record in the dataset. Using the publicly available dataset from the Internet Movie Database (IMDB) as the source of background knowledge, they matched certain subscribers with their Netflix records, and uncovered their apparent political preferences and other potentially sensitive information. The paper also offered a formal mathematical treatment of how a small amount of auxiliary knowledge about an individual can be used to do a fairly reliable re-identification. In the case of the Netflix dataset, the authors \cite{narayanan2008robust} found that with only 8 movie ratings, 99\% of the records could be uniquely identified. Furthermore, they proved that the de-anonymization algorithm they employed is robust to discrepancies in the rating and dates.

\subsection{Group Insurance Commission}
In 1997, Latanya Sweeney wrote a paper on how she had identified the medical records of Massachussets governor William Weld based on publicly available information from the database of Group Insurance Commission. She achieved this analyzing data from a public voter list, and linked it with patient-specific medical data through a combination of birth date, zip code, and gender\cite{sweeney2002k}. As these columns were similar in both databases, their combination could be used to identify medical records that belong to either one person, or a small group of people. Sweeney hypothesized that 87\% of the US population could be identified by having the combination of the three aforementioned records. It's worth noting here that this theory is not conclusive. A paper by Daniel Barth-Jones suggests that the re-identification of Weld may have been a fluke due to his public figure, and that ordinary people risk of identification is much lower\cite{barth2012re}. 

\subsection{New York Taxi dataset} 
The New York City Taxi and Limousine Commission released a dataset in 2013 containing details about every taxi ride that year, including pickup and dropoff times, location, fare, as well as anonymized (hashed) versions of the taxi's license and medallion numbers. Vijay Pandurangan, a researcher for Google, wrote a blog-post where he showed how he exploited a vulnerability in the hashing-function to re-identify the drivers. He then showed how this could be potentially used to calculate any driver's personal income\cite{vijay2014online}. 

Another researcher, called Anthony Tockar, wrote an article during his internship at Neustar Research where he proved that the dataset also contained an inherent privacy risk to the passengers which had been riding New York Taxis. Even though there was no information in the dataset on who had been riding the taxis, Tockar showed that by using auxiliary information such as timestamped pictures, he could stalk celebrities and figure out to where they were driving, and how much they tipped the driver. He also used map data from Google Maps to create a map of dropoff locations for people that had exited a late night visit from gentleman's club and taken a cab home. He then used websites like Spokeo and Facebook to find the cab customer's ethnicity, relationship status, court records, and even a profile picture\cite{tockar2014online}.

\section{Attack Vectors}


\subsection{Linkage Attacks}
Record Linkage
When an adversary tries to link each record in a dataset to 
Attribute Linkage
Table Linkage

\subsection{Background Information}	

\cleardoublepage