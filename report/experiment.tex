	%===================================== CHAP 4 =================================

\chapter{Experiment Planning and Results}
\label{ch:experiments_and_results}

\section{Experiment Plans}
\label{sec:experiment_plan}

In this section we list the set of experiments we have performed in order to answer the research questions as stated in Section \ref{sec:problem_statement}. In the first section, we list all the experiments we have performed, and the parameter configuration we employed for each experiment. The results of these experiments will be provided in section \ref{sec:experiment_results}. When more than one peer was involved in the execution of an experiment, we measured and report the mean accuracy of all peers.


\subsection{Measuring accuracy}
\label{sec:experiment_measuring_accuracy}
The experiments in this section are intended to show the accuracy that results from differentially private model aggregation compared to locally trained models. This experiment is concerned with our goal of validating previous work and how error rates differ in the centralized and the distributed, differentially private setting.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|}
		{\bf Experiment Name}           & {\bf Peers} & {\bf Data per peer} & {\bf $\epsilon$} \\
		\hline
		Centralized logistic regression & 1           & 3000       & N/A              \\	
		Disjoint logistic regression    & 10          & 300        & N/A              \\
		Aggregated model                & 10          & 300        & 1.0              \\
		Ensemble model                  & 10          & 300        & 1.0              \\
		Aggregated model                & 10          & 300        & 0.1              \\
		Ensemble model  & 10    & 300        & 0.1             
	\end{tabular}
	\caption{Measuring accuracy}
	\label{tab:experiment_accuracy_measuring}
\end{table}

In each of the experiments in Table \ref{tab:experiment_accuracy_measuring}, we first chose an optimal regularization $\lambda$ in the range $[2^{-8}, 2^8]$ by cross validation. We then measured the classification accuracy with the chosen $\lambda$ on the test set. In all experiments, the aggregation group size was set to be equal to the number of peers, and $\epsilon_{A}$ was set to be equal to $\epsilon$. This means that there could be produced at most one aggregated model, and it would be available to all the participants.

The \textit{Centralized logistic regression} experiment was intended to establish the best achievable performance with our implementation of logistic regression trained by SGD. It corresponds to the traditional non-private and centralized training of classification models. Note that the results of these experiments may not be state-of-the-art for each data set, since we have not performed advanced feature extraction and selection. This is acceptable, since the intention of these experiments was to establish a baseline that we can compare with when producing models that are formed in a private and decentralized manner. 

The \textit{Disjoint logistic regression} experiment considers a situation where the participating peers have a subset of the data and locally train one model each. Each peer fits a model and makes predictions independently.

The \textit{Aggregated model} experiment let the peers create an aggregate model using \cite{pathak2010diffprivhomo} approach, and the peers use this model only when labeling data. This means that the locally trained model is only used to produce the aggregate model and never for classification. In the \textit{Ensemble model} experiment the peers also produce an aggregate model, but when classifying data their local model and the aggregated model classifies in an ensemble. The \textit{Aggregated model} and \textit{Ensemble model} experiments were run twice, with two different values of $\epsilon$, which represent a significant difference in privacy level.

\subsection{Confirming expected effects of differential privacy}

In this project we have implemented training of a logistic regression, the aggregation mechanism guaranteeing differential privacy and the communication scheme that forms groups of peers to create aggregate models. When tuning the parameters of the implementation, we expected certain changes in the measured performance based on the theoretical and experimental results of previous work. The experiments in this section are intended to be validation of both previous work and of our implementation. In particular, we expected certain effects when changing the privacy parameter $\epsilon$ and the regularization parameter $\lambda$. By confirming the expected behavior, we could increase confidence in the correctness of our implementation, while also visualizing the dynamics of differential privacy.

\subsubsection{Changes in $\epsilon$}

The variance of the noise added when producing aggregated models increases with the parameter $\epsilon$. To confirm this behavior, we ran experiments with all parameters fixed except for $\epsilon$.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\textbf{Experiment Name}            & \textbf{Peers} & \textbf{Data per peer} &
		 $\boldsymbol{\lambda}$ & $\boldsymbol{\epsilon}$                                              \\
		 \hline
		Spam, effect of $\epsilon$ & 10    & 368  & $2^{-2}$  & $[2^{-8}, 2^{8}]$
	\end{tabular}
	\caption{Effects of privacy level}
	\label{tab:experiments_privacy_level}
\end{table}

All the peers in this experiment collaborate to produce one aggregated model, and the full privacy budget is expended in the single aggregation. When the peers are tasked to label the test data, they use their local model and the aggregated model in an ensemble.

\subsubsection{Changes in $\lambda$}

As stated by Equation \ref{eq:aggregated_logistic_sensitivity},  increasing the regularization parameter $\lambda$ will decrease the variance of noise added when aggregating. For this reason we expected that higher values of regularization should help counter the perturbing effect of lower values of $\epsilon$. However, as the regularization value grows, the predictive performance should degrade as the models become unable to fit to the data. To confirm these effects, we tested wide ranges of regularization strength with different levels of privacy.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\textbf{Experiment Name}    & \textbf{Peers} & \textbf{Data per peer} & $\boldsymbol{\lambda}$         & $\boldsymbol{\epsilon}$ \\
		\hline
		Spam, observing $\lambda$, no privacy       & 10    & 368  & $[2^{-8}, 2^{3}]$ & $2^{10}$   \\
		Spam, observing $\lambda$, common privacy   & 10    & 368  & $[2^{-8}, 2^{3}]$ & $0.1$      \\
		Spam, observing $\lambda$, stronger privacy & 10    & 368  & $[2^{-8}, 2^{3}]$ & $0.01$    
	\end{tabular}
	\caption{Effect of regularization strength}
	\label{tab:experiments_regularization_strength}
\end{table}

The values of $\epsilon$ for the  common privacy level in Table \ref{tab:experiments_regularization_strength} is chosen based on \cite{dwork2008differential}, which suggests that $0.1$ is a common value.

\subsection{Changes in data availability}

As we are testing in a decentralized setting, we wanted to compare how the local, the aggregated models, and both of them in an ensemble respond to changes in data availability.  The experiment seen in Table \ref{tab:experiments_data_availability} is concerned with exploring the way data availability affects error rate in the distributed setting. As all other parameters except for the amount of data per peer is fixed, the overall size of data in the system changes for each parameter combination in this experiment.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\textbf{Experiment Name}                                 & \textbf{Peers} & \textbf{Data per peer} & $\boldsymbol{\lambda}$  & \textbf{Type}       \\
		\hline
		Spambase, data availability, disjoint         & 10    & $10-360$  & $2^{-2}$       & Local      \\
		Spambase, data availability, aggregated    & 10    & $10-360$  & $2^{-2}$       & Aggregated \\
		Spambase, observing $\lambda$, ensemble & 10    & $10-360$  & $2^{-2}$   & Ensemble  
	\end{tabular}
	\caption{Effect of data availability}
	\label{tab:experiments_data_availability}
\end{table}

\subsection{Changes in number of participants}

 
The experiment shown in Table \ref{tab:experiments_peer_numbers} is an attempt to partially answer to our research question about the possible loss of accuracy in our distributed setting from the participation perspective. As we expected the error rate might be affected by the number of participating peers, we designed this experiment to give an idea of how peer prediction changes when more peers are present. Except for the number of peers, all parameters were kept fixed, including the amount of data per peer. This means that as the number of peers increase, there is more data available in the system.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|l|}
		\textbf{Experiment Name}                & \textbf{Peers}      & \textbf{Group size} & \textbf{Data per peer} & $\boldsymbol{\lambda}$ & $\boldsymbol{\epsilon}$ \\
		\hline
		Adult, increasing participants & $5-50$ & 5          & 500  & $2^{2}$   & $1.0$     
	\end{tabular}
	\caption{Effect of number of peers}
	\label{tab:experiments_peer_numbers}
\end{table}

\subsection{Peer error rate variance}
\label{sec:experiment_peer_variance}
With this experiment we wanted to explore the variance in the quality of models held by each peer, and see how it changes when aggregate models are introduced. In order to increase the chances that we can observe variance among peers, the amount of data owned by each peer is set at a low level of 250.

This experiment was concerned with reduction of peer error rate variance and to some extent validation of previous work. Concerning the latter, we hoped to show whether or not there was benefit in differentially private model aggregation compared to each peer using a local model. Concerning the former, we wished to see if publishing aggregated models and using them in ensembles to make predictions could help reduce peer error rate variance.
 
\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|l|}
		{\bf Experiment 	Name}                  & {\bf Peers} & {\bf Data} & $\boldsymbol{\lambda}$ & $\boldsymbol{\epsilon}$ & {\bf Type} \\
		\hline
		Adult, peer variance, only local       & 10          & 250        & $2^{2}$   & $1.0$      & Local      \\
		Adult, peer variance, aggregated       & 10          & 250        & $2^{2}$   & $1.0$      & Aggregated \\
		Adult, peer variance, ensemble of both & 10          & 250        & $2^{2}$   & $1.0$      & Ensemble  
	\end{tabular}
	\caption{Observing peer accuracy variance}
	\label{tab:experiments_peer_variance}
\end{table}

\subsection{Effect of aggregation group size and model propagation}

We believed that the number of peers participating in creating aggregated models could affect the quality of the produced models. In the experiment seen in Table \ref{tab:experiments_group_sizes} we have a fixed number of peers and a fairly high level of $\epsilon$. The privacy level was chosen from the high end of candidate values. This is because we want to observe the value of aggregating models, which should be more apparent in a situation with lower perturbation. Since participation in a single model aggregation fully expends the privacy budget of a peer and the number of peers is fixed, a higher number of aggregate models are produced when the group size is smaller. Smaller groups result in each peer having a larger ensemble with each model being based on less data. Larger groups result in each peer having a smaller ensemble with each model being based on more data.

This experiment relates to two of our research questions - reduction of accuracy loss and peer accuracy variance in our distributed, differentially private setting. We hoped to identify group configurations that minimized both of these performance metrics.

We run this experiment with two different approaches for publishing the aggregated models, as discussed in Section \ref{sec:PropagationPubModel}.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		{\bf Experiment Name} & {\bf Peers} & {\bf Group size} & $\epsilon$ & {\bf Publication} \\
		\hline
		Changing group sizes  & 30          & $[1, 5, 10, 15, 20, 25, 30]$      &     $1.0$ & Party     \\ 
		Changing group sizes  & 30          & $[1, 5, 10, 15, 20, 25, 30]$      &     $1.0$ & All     \\ 
	\end{tabular}
	\caption{Effect of aggregation group size}
	\label{tab:experiments_group_sizes}
\end{table}


\subsection{Value of budgeting privacy}
 
As discussed in Section \ref{section:privacy_budget}, it is possible to spread the usage of the privacy guarantee in a budgeted fashion. Table \ref{tab:experiments_budgeting_privacy} shows the experiment where we wanted to explore the potential benefit of performing repeated aggregations, at the cost of lower $\epsilon$ in each aggregation. 

This relates to our first research question, with considers loss in accuracy, in two ways. Firstly, creating ten aggregated models by spreading the budget and then using them in an ensemble might on its own have interesting effects on quality of predictions. Secondly, it might be necessary for peers in a distributed setting to participate in aggregations several times. This is especially true if aggregated models cannot be shared globally for privacy or practical reasons.

\begin{table}[h]
	\centering	
	\begin{tabular}{|l|l|l|l|}
		{\bf Experiment Name} & {\bf Peers} & $\boldsymbol{\epsilon}$ & {\bf Aggregation cost}        \\
		\hline
		Budgeting privacy & 10    & 0.1     & $[\frac{0.1}{16}, 0.1]$
	\end{tabular}
	\caption{Effect of budgeting privacy}
	\label{tab:experiments_budgeting_privacy}
\end{table}
