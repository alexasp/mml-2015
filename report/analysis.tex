%===================================== CHAP 5 =================================

\chapter{Analysis}
\label{ch:analysis}
In this chapter we will give our analysis of the experimental results seen in Section \ref{sec:experiment_results}.

\section{Comparing Measured Error Rate}
\begin{table}[h]
	\begin{tabularx}{\textwidth}{|X|l|}
		\textbf{Experiment and author}                                             & \textbf{Error Rate}      \\
		LOGICBOOST\citep{sharma2013adaptive}                                       & 0.1024 \\
		LogReg-TRIRLS\citep{kumar2012comparative}                                  & 0.1389 \\
	\end{tabularx}
	\caption{Table with baseline results from the Spambase Dataset}
	\label{tab:baseline_class_results_spambase}
\end{table}

\begin{table}[h]
\begin{tabularx}{\textwidth}{|X|l|}
	\textbf{Experiment and author}                                                 & \textbf{Error Rate}      \\
%	Krempl,Decision tree with bagging\citep{}                                    & 0.196 \\
	Optimal Logistic regression \citep{caruana2006empirical}   				       & 0.114 \\
	Multiparty DP LogReg \citep{pathak2010diffprivhomo}                            & 0.24 \\	
\end{tabularx}
\caption{Table with baseline results from the Adult Dataset}
\label{tab:baseline_class_results_adult}
\end{table}

	

These two tables form the baseline for the analysis of our classification framework. In Table \ref{tab:baseline_class_results_spambase} we have included the results of two classifiers reported in the literature \citep{sharma2013adaptive,kumar2012comparative} for the Spambase data set. These are both centralized approaches to logistic regression, where both had as a research goal of trying to find an ideal classifier for spam detection. In similar fashion, we provide two baseline classification results in Table \ref{tab:baseline_class_results_adult} for the Adult data set. Here we chose to provide baselines for two approaches, where the first \citep{caruana2006empirical} is an optimal result using logistic regression in a centralized manner. The second entry is the result of the differentially private system of \cite{pathak2010diffprivhomo}, on which we've stated that we wish to improve in RQ3.

An immediate surprise we should point out is that \cite{pathak2010diffprivhomo} gives the error rate of their optimal, centralized solution applied to the Adult set as $0.24$. This is strange, since it is very close to the class distribution of the Adult data set. While they show that their differentially private solution approaches their centralized solution, they give no compelling reason to believe that the error rate of $0.24$ is a non-trivial performance. A very similar error rate could be achieved by only predicting the negative class.

We have restricted our analysis to the error rates of our solution, though confusion matrices and ROC-curves were produced for all experiments. The confusion matrices were used to verify that the classifiers were discriminating between the two classes. While error rates are presented as mean value among all peers, we give only the confusion matrix of a single, randomly selected peer. The intention is that the error rates we report reflect the model quality on average for all participants in the system, while the confusion matrices are intended to support the claim that the reported error rates correspond to non-trivial classifiers. Confusion matrices for all the data sets are included in the Appendix.


\begin{table}[h]
	\centering
	\begin{tabular}{llll}
		& \multicolumn{3}{c}{Actual}                                                      \\
		\multicolumn{1}{c}{} &                        & 1                         & 0                          \\ \cline{3-4} 
		Predicted            & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{2393} & \multicolumn{1}{l|}{1146}   \\ \cline{3-4} 
		& \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{1453} & \multicolumn{1}{l|}{11289} \\ \cline{3-4} 
	\end{tabular}
	\caption{Confusion Matrix: Adult. Local model only.}
	\label{fig:confmat_adult_local}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{llll}
		& \multicolumn{3}{c}{Actual}                                                      \\
		\multicolumn{1}{c}{} &                        & 1                         & 0                          \\ \cline{3-4} 
		Predicted            & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{2245} & \multicolumn{1}{l|}{888}   \\ \cline{3-4} 
		& \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{1601} & \multicolumn{1}{l|}{11547} \\ \cline{3-4} 
	\end{tabular}
		\caption{Confusion Matrix: Adult. Aggregated, DP model only.}
		\label{fig:confmat_adult_aggdp}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{llll}
		& \multicolumn{3}{c}{Actual} \\
		\multicolumn{1}{c}{} &                        & 1                         & 0                          \\ \cline{3-4} 
		Predicted            & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{2102} & \multicolumn{1}{l|}{895}   \\ \cline{3-4} 
		& \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{1744} & \multicolumn{1}{l|}{11540} \\ \cline{3-4} 
	\end{tabular}
		\caption{Confusion Matrix: Adult. Ensemble of Aggregated and Local.}
		\label{fig:confmat_adult_ensemble}
\end{table}

Another important point to discuss in this context is validation at the peer level. How would a peer know if its local model or ensemble is good enough? We have validated with cross validation and tested with a held out test set, but this luxury is not available in a real setting. One option would be to design a scheme to perform cross validation at the aggregation group level.

Ideally, peers could cooperate to form a sort of test set when the system is live by sending incoming data instances to each other. This would allow the system to respond more quickly to changes in data distribution, as deterioration in prediction accuracy can be measured in a near-global fashion as soon as the changes occur. This approach is problematic, since peers may want to keep all incoming data private. The other alternative would be to have peers measure their performance on their respective incoming data only. If peers have very different incoming data distributions, this would give a less biased estimate over time, but building a good estimate could take much longer, depending on the data rate.


%\todo[inline]{Expand upon our own baseline results, and compare them to the baselines found in literature}
%\todo[inline]{Add a segway onto the next sections, so that we can discuss the importance of the different parameters in a natural way}

\section{Importance of Epsilon}
As explained in section \ref{section:differential_privacy}, differential privacy works by disguising an individual's data in a data set by adding noise to their records. The amount of noise added is determined by the privacy parameter  $\epsilon$, and grows exponentially the closer the parameter gets to zero. 

 
Figure \ref{fig:epsilon_big_range} shows the effect of the privacy parameter $\epsilon$ in our experiment. We wanted to test the effect of varying the $\epsilon$-value in the range from $2^{-10}$ to $2^9$, especially to find out how the classifier would perform when faced with data with high amount of noise added to it. The positive class rate in the UCI Spambase data set is 0.4, so any error rate at this level is no better than a random classifier. The plot shows how sensitive output is to the values of $\epsilon$.

Our experiments indicate that an ideal range for the $\epsilon$-value seems to be between 0.1 and 1.0 for all datasets. If the value falls below this range, both the error rate and the error variance quickly grows and the classifier becomes unusable. Some of the earlier research by \cite{dwork2008differential} theorized that 0.01 might be suitable in analysis with a need for an extra strong privacy guarantee. In our case that is not a feasible limit, as the error rate at this privacy level is growing towards the class distribution. Other research performed in the literature \cite{pathak2010diffprivhomo,kellaris2013groupingSmoothing,hsu2014economicEpsilon} seem to agree that 0.1 is a natural lower limit.   

Looking at Table \ref{tab:results_measuring_accuracy}, we see that the difference between the our optimal result from Adult at 0.157 with $\epsilon$=1.0 and the high privacy result of 0.163 with $\epsilon$=0.1, is not particularly big. The results of running a big range of epsilon over the Spambase dataset (Figure \ref{fig:epsilon_big_range}), also indicate that there is not much improvement in increasing the $\epsilon$ beyond 1.0. This is interesting, as it suggest there might exist a natural bound on the trade-off between privacy and utility. If a theorem could prove this bound of where there will be no further increase in utility by decreasing the privacy, this would serve as a big breakthrough for the field of differential privacy. To the best of our knowledge however, no such theorem has yet been found. \cite{lee2011epsilonBound} have tried providing such a bound, but their proof is limited to a very specific scenario on a unreasonably simple example database. Nevertheless, their effort suggest that future research might bring more conclusive proof. In our case, we can only report what we have discovered after performing carefully monitored experiments.   


\section{The Importance of Data} \label{importance_of_data}
As can be told from Figure \ref{fig:data_limit_test_localmodelonly}, \ref{fig:data_limit_test_withlocalmodel} and \ref{fig:data_limit_test_localmodelonly}, the amount of data available for each peer is important. The more data a peer have, the greater the chance is that it will make a decent local classifier. 


Figure \ref{fig:data_limit_test_withlocalmodel} and \ref{fig:data_limit_test_withoutlocalmodel} indicate how the classification performance improves as the amount of data records available to each peer grows. When each peer only have a small amount of data to create their local classifier, the classifier tends to have display terrible performance. As peers gain more data, both the performance and the variance of the classifier improves.     

The reason for this improvement is two-fold. 1: A bigger sample size for the logistic regression model generally leads to better performance \cite{peduzzi1996simulation}. 2: The sensitivity of logistic regression (see equation \ref{eq:logres_sensitivity}) is bounded by the size of the training set. What this means is that the more data a peer have available, the less amount of noise is needed to obfuscate their logistic model. 

The observation we've made is therefore thoroughly grounded in theory, and is important to highlight when discussing our main research question. Until a certain amount of data has been gathered, a system based on our distributed architecture will display very poor performance. What is interesting however, is that the amount of data needed seems to be a relatively low number. In the paper written by \cite{pathak2010diffprivhomo}, they report that each party was given at least 3256 data records. In our experiments we found that a much smaller amount of data could still result in decent classifiers. 	  	

\section{Importance of Regularization}

\subsection{Regularization in a low privacy setting}
Figure \ref{fig:regularization_extremelyhighepsilon} shows the normal effect regularization has on accuracy for the Spambase data set, by setting $\epsilon$ so high that noise is essentially nonexistent. As the regularization parameter $\lambda$ grows large, the model becomes less able to fit the training data, eventually resulting in models predicting only the negative class, which constitutes 60\% of the data set. This happens because the high regularization forces the parameter vector to the zero vector, resulting in uniform class probability for all samples. 

On this particular data set it appears that a logistic regression model has low risk of overfitting, since the cross-validated error does not increase when the level of regularization is very low. Ignoring the effects of privacy mechanisms, this would mean that selecting some regularization parameter in the lower half of the range should offer reasonable performance. Choosing a level at the high end of this range could still be preferable, motivated by a desire to attain a model that is highly generalizable.

\subsection{Regularization in a high privacy setting}

When noise with significant variance is added to the model creation process, tuning $\lambda$ will adjust noise variance as well. Equation \ref{eq:aggregated_logistic_sensitivity} states that the noise variance is inversely proportional to $\lambda$. The choice of regularization then must balance the model flexibility at lower levels of $\lambda$ with the decreased noise at higher levels of lambda. When epsilon is reduced, and therefore noise variance increased, the choice of regression becomes more clearly restricted than in the noise-free situation. Based on figure Figure \ref{fig:epsilon_big_range}, we picked $\epsilon = 0.1$ to attempt to visualize this effect. At this level of privacy we see a clear decrease in performance, while the models still are making better than random predictions. Figure \ref{fig:regularization_normalepsilon} shows mean error rate when $\epsilon$ is set to this level of privacy. The effect of noise is now clearly visible far into the range of regularization, only returning to comparable error rate when $\lambda = 2^{-2}$. 

A further decrease in $\epsilon$ should shift the optimal regularization level even higher, eventually meeting with the point. This situation is shown in Figure \ref{fig:regularization_lowepsilon}, where the experiment moves into lower variance just as regularization is getting too high to support good model fit. The result is that there is no good value for $\lambda$. A situation like this could be remedied by either decreasing the level of privacy, or increasing the minimum amount of data available at the peers, both of which would decrease noise variance as given by Equation \ref{eq:aggregated_logistic_sensitivity}. This could be something to keep in mind if putting a differential privacy method into practice, whenever the sensitivity depends on data set size - postponing model training to a later date when more data is available can be beneficial for both model generalization and minimizing the impact of privacy.

\subsection{Thoughts and guidelines on regularization}
\label{sec:reg_thoughts_guidelines}
The fact that regularization level affects the noise variance has another interesting consequence when tackling more than one data set. Typically, different data sets tolerate varying levels of regularization. For the Spambase data set, Figure \ref{fig:regularization_extremelyhighepsilon} indicates that $\lambda$ should be at most $2^{-2}$ or $2^{-3}$, while the Appendix figures for the Adult data set with same per-peer record count show near optimal regularization values as high as $2^2$. Such a significant difference in regularization can make or break a model, due to the effect it has on the noise variance.

The method we are testing have a couple of notable aspects concerning selection of regularization strength. Firstly, it is necessary to take care to validate models in a way that takes into account the increased variance in trained models due to noise.

Secondly, the fact that regularization has such a significant effect on the noise variance means that truly knowing the error rate of the final model would require aggregation and noise addition. Indeed, this issue arises in both the original approach by \cite{pathak2010diffprivhomo} and our method. Since noise is not added until the aggregate models are produced, a realistic measurement of performance can't be performed until after the aggregation step. This means that either the privacy budget would be expended in the first iteration of cross validation, or the noise level for each aggregation would have to be reduced. In our experiment we have determined good values of $\lambda$ by performing grid searches over ranges of values, but doing a grid search in a decentralized setting while maintaining privacy guarantees is non-trivial. It would be necessary to create a new protocol which includes some method of $\lambda$ selection. 

We believe the most practical way to do this would be by letting each peer choose the strongest possible regularization that is within some bounded distance from the optimal value found in cross-validation on their local data. The protocol by \cite{pathak2010diffprivhomo} briefly described in Section \ref{sec:Sensitivity_of_LogReg} also includes a step to secretly communicate the minimum data set size among all the participants. The peers could additionally communicate their chosen $\lambda$ and the curator could then use the minimal regularization level when choosing the level of noise to add.

\section{Analysis of Peer Participation Effects}
\label{sec:aggregation_effects}

The previous paragraphs considered effects of noise variance on the ability of peers to make predictions. In this section we consider another important aspect that determines the quality of aggregated models - the number of peers that participate to create them. 

Figure \ref{fig:peer_range_constant_group} shows what happens when the aggregation group size stays fixed, but different numbers of peers are available. A higher number of peers mean that more models can be created. In this experiment the aggregation group size was five, which means that just one aggregated model was produced at the low end of the range, while 10 models were produced at the high end. If there is any utility of aggregating models, it is not visible in the mean error rate for the Adult data set with this amount of records. We suspected that an effect would be stronger when the peers have less data, since aggregation should allow the peers to benefit from each other data. We tested with different amounts of available data as low as 75 instances per peer, but we made the same observations in these experiments. The charts showing other levels of data availability are included in the Appendix. 
% \todo[inline]{Add figures to appendix that show our other tests}.

One possible explanation for the small changes in error rate might be the uniformity of data distribution. In our current experimental setup, the data is partitioned randomly between the peers, which means that the peers will have same number of samples from the same data distribution. In reality, users might have very different numbers of samples with different distributions. This is discussed further in Section \ref{sec:Future Work} on Future Work. On the other hand, these experiments don't show any negative effects of predicting with an ensemble of aggregated models. While we expect that there are situations where ensemble prediction could be detrimental, such issues could be mitigated by actively removing less useful models from the ensemble as it grows.

\section{Analysis of Model Variance}

While it is an important goal to minimize error rates, the variance in performance across peers might be just as important. It could be better that all participants achieve sufficient predictive performance rather than having some peers with fantastic performance while others make useless predictions.
 

We see from the results in Figure \ref{table:peer_variance_adult} that variance is smaller when the aggregated models are used, but the difference is not very big, as the variance is small in both cases. We believe the weakness of the effect might be for the same reason as the small amount of change in error rate seen in Section \ref{sec:aggregation_effects}. The data is distributed uniformly to the peers, which means that they will get fairly similar models as long as they have sample data to get close to what a model trained on the full data set would look like. Given a data set partitioned into less uniform subsets, the model variance might increase and the beneficial effect of aggregations could become more apparent. However, this would require first attaining and preprocessing a suitable dataset. It would also require additional changes such as implementing minimum data set sizes for participation, to avoid excessive noise being added in aggregation. Due to limited time, this is left for future work.

%below is old version
\begin{comment}
\section{Analysis of Propagation Effects}

This must be updated if new propegation experiments are added

As mentioned in Section \ref{sec:PropagationPubModel}, we hypothesized that we could improve the overall classification accuracy of our system by publishing the aggregated models generated. In this section we present the effects of publishing method. Figure \ref{fig:RegRangeTestPubParty} shows the case where each group of peers only share the perturbed, aggregate model among themselves, while Figure \ref{fig:RegRangeTestPubAll} shows the results the model is sent to all existing peers. 

The most obvious effect of globally publishing models is that the standard deviation is much lower than the group publishing case. This is not surprising.

The truth is probably somewhere in the middle - we can't expect to easily publish models globally in all settings. Additionally, there might be situations were global publishing could be detrimental to real world performance - for instance, if there are strong geographical, temporal or demographic trends, it might be better to limit the amount of model sharing to suitable subsets.

\todo[inline]{Add figure to results that deal with comparing party vs all}

As the previous experiment indicates that publishing newly made models to as many peers as possible is better \todo{Add a reflection talking about why this makes sense.}, we performed an additional experiment to determine whether it in such a scenario would be better to perform many aggregations with fewer models included in each aggregate or performing few aggregations including many models. This was achieved by testing performance with a range of values. In the experiment, each peer can only participate in a single aggregation before reaching the limit set by the privacy guarantee. For example, given a set of 50 peers, a aggregation size of 25 can only publish two aggregated models.

\end{comment}

\section{Analysis of Aggregation Groups Size and Model Propagation}

The results of this experiment are seen in Table \ref{tab:results_groupsize_party} and \ref{tab:results_groupsize_all}. In these experiments we measured how the mean error rate changes as the size of the aggregation groups vary. We ran two such experiments, one where aggregated models were shared globally with all peers and one where they were shared only with the party that participated in producing each model. Each peer will include in its ensemble any model shared with it.

Firstly, note that an aggregation group size of one corresponds to each peer simply fitting a model and publishing it with noise added. It seems that there is at least some benefit of producing aggregates, as a group size of one has the highest error rate. There also appears to be some positive trend for higher group sizes in the Party-publising case. Unfortunately, since the different is quite small, a certain conclusion cannot be drawn for this particular data set.

While the difference in error rates is too small to be significant between the two approaches of publishing models, there is a significant effect on the standard deviation of the error between iterations of the experiment. Though the scale of standard deviation is very small for both cases, this indicates that globally sharing models might be preferable. Repeating these experiments on data sets less uniformly distributed among peers could increase scale of the variance, increasing the importance of sharing models globally. Of course, globally publishing all models might not be feasible for practical reasons or acceptable for privacy reasons. Instead, spreading the aggregated models as widely as possible might achieve a similar effect.

Next, we consider the standard deviation, or error variance, measured among the peers. This metric indicates how consistently the network achieves good predictive models. Firstly, we note that publishing to all peers yielded much lower standard deviation for most group sizes. Again, the scale of variance is fairly small, but we believe this effect could be much more considerable with other data sets, especially if data is not uniformly distributed. Interestingly, the variance in peer error rate is smallest when aggregation group sizes is small. This means it might be necessary to make a trade-off between mean error rate and variance between peers.

This effect might be caused by the fact that the peers include their locally fitted models in their ensemble when classifying 
data. As seen from the results in  Section \ref{sec:results_peer_error_variance}, the variance in error rate among peers is zero for aggregated models. This is as expected, since all peers share the exact same set of models. The local models on the other hand result only from the local  data of each peer. Since all models in the ensembles are weighted equally, the effect of the local model decreases as a higher number of aggregate models are published. If this is the case of lower peer error rate variance at lower group sizes, a similar decrease in variance might be achieved by decreasing the weight of the local model. On the other hand, this might be detrimental for those peers that succeeded in training high quality models. In an online learning setting, models weights could be learned over time as new labeled data arrives.
\begin{comment}ly

A possible explanation of this result is that the effects of \unsure{bagging?} boosting counters the loss in accuracy that results from the addition of noise. Since models will have noise added to them before being published, expending all data to produce a single model might yield a worse classifier than partitioning data, adding noise to each separate, weaker model and combining them in an ensemble. On the other hand, this effect could be caused by some leakage of privacy which becomes visible over repeated applications of the aggregation mechanism. \todo{Figure out if there is some way we could test or prove that this is not the case.} 

The interesting observation that can be made from this is that the best situation is when there is no aggregation. A group size of one results in only a single model being contributed, and is equivalent to each peer publishing its local model with noise. One possible reason for this could be that there simply is no value in averaging models in the way done in our experiments and by Pathak et al.\cite{pathak2010diffprivhomo}. Neither our experiment or the experiment by Pathak et al. help distinguish between these two possibilities. The experiment by Pathak et al. only demonstrate that their method for creating aggregated models has comparable performance to adding noise to a centrally computed model. Additionally, this is only demonstrated with large data sets. In their experiment, the minimum data set size for any participant is 3256. With data sets this large, it is possible they would have gotten similar results by testing a model produced by a single participant, without performing additional aggregation. However, no experiment evaluating this possibility remains. Their theoretical conclusions stand, but experimentally validating the value of aggregation is necessary.

Thus the key question is whether or not aggregation is worth the complexity of a homomorphic encryption protocol. If similar performance can be achieved solely by ensemble classifiers of differentially private models published by each peer, one could skip the complexity and risk of relying on a cryptographic protocol to maintain privacy.

Another possible explanation for this observation is that aggregation might be useful, but not when the peers all have samples of data from the exact same distribution. This is the case with the Spambase data set used in the experiment in Figure \ref{fig:groupsize_is_better}.

One way to answer question would be by finding a data set which has subsets that are produced by distinct distributions, and partitioning data by source distribution. Due to time constraints we did not have time to locate and prepare data sets that fit this requirement. \todo{See if we have time to do this. We REALLY sho1uld, otherwise it will very possibly give a lower grade.}

\todo[inline]{Add figure to results that deal with effect of various budget divisions}

An alternative avenue for validating the value of doing aggregation could be to reduce the amount of data given to each peer. If just one peer has data sufficient to train a good classifier, it is sufficient. For that reason it makes sense to stage a situation were it is highly unlikely that even a single peer gets a lucky subset of data. Figure \ref{fig:groupsize_limiteddata} demonstrates such a case. Again, there is not clear indication that averaging multiple models is better than simply publishing them individually and using them in ensemble classifiers.
\end{comment}

%\subsection{Issues with cold start}
%
%Section \ref{sec:} important motivation for this project was the idea that application users should be able to retain ownership of their own data, and only share it to a degree that they choose.
%
%
%\todo[inline]{Explain what cold start issues is. Discuss how this is very prominent in our sitaution, and the different ways we can deal with the cold start, and pros and cons of those approaches. Essentially, we have already experimented with a kind of cold start situation when we try to performance numbers for peer\_count 1 to 50}
%




\cleardoublepage