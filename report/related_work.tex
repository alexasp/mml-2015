
\chapter{Related Work} \label{ch:Related Work}

In this chapter we will present some of the existing works related to our thesis and research goals. The papers referenced in this chapter are naturally also related to this thesis' constituent papers. We have chosen to mention papers that are related to one or more of the major theoretical framework we have employed. This chapter will therefore be structured in three sections: the first will give the main theoretical contributions to the concept of differential privacy. The second section will explore some centralized approaches to machine learning with differential privacy guarantees. The third and last section will explore works that have employed a distributed approach similar to our own framework.   

\section{Differential privacy}
As mentioned in Chapter \ref{ch:Basic Theory}, differential privacy was defined by Cynthia Dwork in her seminal work published in 2006 \citep{Dwork06differentialprivacy}. This paper lay the mathematical foundations for the privacy guarantee we employ in our work. Dwork later expounded on her work in the book Algorithmic Foundations of Differential Privacy\citep{dwork2013algorithmic}, which we used as a main piece of reference when gathering knowledge in the early phases of this thesis project.

Other defining works include the two papers written by Frank McSherry. His work on mechanism design \citep{mcsherry2007} allowed for the expansion of Dwork's Laplacian mechanism design, by providing the theoretical analysis that other mechanisms could satisfy the same guarantee (see section \ref{sec:Exponential Mechanism} for more). His paper \citep{mcsherry2009PINQ} on the design and implementation of PINQ were hugely influential in the early phases of our project, as we could use the paper and the publicly available code when implementing our own design. 

\section{Centralized approaches}
In the years following the release of Dwork's seminal paper there have been a steadily increasing amount of publications, reaching a peak in 2013 with 141 papers published and indexed by the Scopus scientific database. Some of these works have focused on the same area as us, namely classification using logistic regression, but instead opted to focus on a centralized approach. Mentioned below are the work which have either influenced our own, or perfomed similar experiments. 

\todo[]{Mention the objective perurtbation approach suggested by Chaudhuri, and why it isn't really suitable for a Pathak-style solution.}

Chaudhuri and Monteloni designed a logistic regression algorithm which guaranteed differential privacy in 2009. They also provided a mathematical proof for the upper bound of the sensitivity of logistic regression (see Section \ref{sec:Sensitivity_of_LogReg}), which formed the basis of our own solution. The same authors provided a follow-up paper\citep{chaudhuri2011riskMinimization}, in which they further developed a method called object perturbation to add noise to the regularized objective function. Their results which showed that objective perturbation is generally superior to output perturbation has proved very useful to the field of differential privacy. 

Zhang et al \citep{zhang2012functionMechanism} further improved upon Chaudhuri's work by creating a new functional mechanism for objective perturbation, which they tested on a set of census data by employing both linear and logistic regression. 

\section{Distributed approaches}
As our research goal states, we wish to create a framework to test the feasibility of employing a distributed, differential private learner. One of the first works in this field was performed by Pathak et al \unsure{is this correct reference formatting?} \citep{pathak2010diffprivhomo}, who proposed a privacy-preserving protocol for composing a differentially private aggregate classifier. Their protocol trained classifiers locally in different parties, and the parties would then interact with an curator through a homomorphic encryption scheme to create a perturbed aggregate classifier. We took inspiration from their protocol when we created our own ensemble classifier, extending the work of Pathak et al. in several ways. We've taken steps to ensure better scalability by adding better group forming for each of the peers, and we've added an publishing step to the aggregation mechanism which allows for the creation of an ensemble classifier in each peer. Lastly we've also performed more extensive experiments to validate the employed method, as Pathak et al. seemed to have focused more on the theoretical side of the experimentation. 

Since the work of Pathak et al was presented in 2010 there have been some research published on how to create private distributed learners. One such example is the work of Boutet et al.\citep{boutet2013DisCollFil}, who presented a privacy-preserving distributed collaborative filtering scheme which relied on user profile obfuscation and randomized response. Another interesting paper is the work of Zhang et al \citep{zhang2014locationRecommendation}, which investigate mechanisms to sanitize location data used in recommendation system with the help of differential privacy. 

Rajkumar and Agarwal \citep{rajkumar2012differentially} presented an alternative to Pathak's method in 2012. It works in a multiparty setting by using a stochastic gradient descent based procedure to directly optimize the overall multiparty objective rather than Pathak's method of combining classifiers learned from optimizing local objectives. Their algorithm achieves a slightly weaker form of differential privacy than that of Pathak et al., but is more robust to the number of parties and the relative fractions of data owned by the different parties.

Ji et al. \citep{ji2014DisLogReg} recently proposed a distributed solution using logistic regression, which learned from both private and publicly available medical datasets. Their solution differ from our own as they employ a globally synchronized structure, whereas our own solution works asynchronously. They also design a mechanism which first uses public datasets to compute the gradient without any form of noise addition, and then perform a distributed logistic regression step with differential privacy. 


