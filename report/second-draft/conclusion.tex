%===================================== CHAP 5 =================================

\chapter{Conclusion}


\section{Threats To Validity}

\subsection{Platform}
A potential threat to the validity of our conclusion/work \todo{Fix this sentence}, is how we performed the setup of the Jade platform. Since we wanted to perform our experiments on over a range of parameters, we needed to find a way to reset the platform after a successive experiment and re-run it with a new set of configurations. We solved this by having a jade agent called CompletionAgent be responsible for waiting for every peer to message indicating their completion, which would trigger the CompletionAgent to deregister all the peers from the MainContainer and then reset the whole environment. The environment would then be set up again with new parameters. 

What we see as a potential source for concern in this process is the possibility for error during the deregistration. During the implementation of this process we encountered some problems in making it work, as the CompletionAgent seemed to take an unreasonable amount of time in completing its purpose. Although we found a solution to this problems, there is still a risk that peers do not deregister as they should and carry through into the next iteration of testing. This could lead to false information being injected into our experiment, which would skew our results.  

We have however minimized this risk by continuously developing unit test to verify new code additions, as well as using JADE's native GUI to supervise the behavior of the peers while running. We therefore conclude that the risk is negligible.

\subsection{Resource Consumption}
A clear weakness of our system is our lack of formal analysis of resource consumption and scalability. In 
\todo[inline]{What have we done to to guarantee scalability? Is it enough to say it should scale well?}

Due to time constraints, we've had to take certain shortcuts while implementing our systems. The most glaring liability for system scalability is our use of a single GroupFormingManager to handle allocating aggregation groups for the participating peers. This manager would quickly become a bottleneck in our system if we wanted to scale the amount of peers beyond just a small mass of users. Our solution to this predicament is found in section \ref{sec:Future Work}, where we provide a solution in the form of the Newscast algorithm. 

\subsection{Homomorphic encryption}
As mentioned in Section \ref{sec:homomorphic_encryption}, homomorphic encryption is still in an infantile stage of development and therefore cannot be called a well-proven technology. Our method for aggregating models from various peers is based on a homomorphic encryption scheme developed by Pathak et al. but due to time constraints we could not actually implement it and instead had to opt for simulating the results of applying this scheme. We therefore do not have real-world results that can validate the applicability of this scheme, nor do we know if applying this scheme would lead to increased run-time and resource consumption. While this remains an interesting area for future research, as it stands now it remains a possible threat to the validity of the results we've achieved and therefore also the conclusion we have drawn from them.   


\section{Future Work} \label{sec:Future Work}

Further develop and test the propagation of aggregated models. We experienced that when we shared the aggregated models globally in our network, we could decrease the SD in our classification error, as well as sometimes improving the classifier. Further research should go in expanding this behavior, as you could potentially propagate models only to peers in geographic and/or demographic vicinity. This could possibly lead to more specialized models, which could give better classification rate to a specialized subset of peers. 

Another important area of research would be to further test the applicability of peers sharing data to create better aggregated models. Our original research questions was designed to explore the validity of our proposed method of doing differentially private machine learning, and our current research has been limited to testing on a small amount of datasets which is publicly available. \todo{text} In the future more research is needed on datasets with an uneven underlying distribution, which could potentially provide results highlighting the usefulness of sharing information between peers. An ideal dataset would be one where each peer only holds data which makes up only a part of the solution. \todo{This last sentence is crap and I hate it}

Implement the Newscast algorithm for selecting peers. The Newscast algorithm is a gossip protocol which facilitates a robust spread of information. The core of the protocol involves periodic and pairvise interaction between processes. Implementing this algorithm would allow our system to scale better when a big number of peers are added to the network. The biggest bottleneck of our system at the moment is the peer sampling during the group forming, as it requires a single agent to act as a manager for how groups are formed. The basic idea of the Newscast algorithm is that each node, or peer in our situation, has a partial view of the system. All nodes exchange their views periodically, which allows them to keep an up-to-date view locally and spread their information throughout the network. Further research into this algorithm would allow us to customize this algorithm so that peers in our network could form groups based on their partial views of the network. 

Full data protection for each peer's data. This would involve dividing the epsilon by the biggest dataset size, as formalized by Dwork in \todo{citation}. This is an ever tighter privacy guarantee, but it would potentially mean that the results would contain too much noise. To test this we would need a massive dataset, as we would need to test the correlation between dataset size, and amount of noise added to each peer. (More noise needs more data to smooth out.)

Real world case which takes humans into account. Right now research in privacy is all about the technical details, and try to get it as close as possible to existing methods. Without some kind of popular support, the method will never see practice in real-world applications. 

Work on a system that would work in a online setting. It could potentially improve the system, as you would have new data coming in which could replace old data with spent budgets, but it would also be potentially a big tradeoff as you won't have the same data history as you would have in a system without differential privacy. Dwork has written about this in her book, so we can take inspiration from there. (As well as our own paper)

Security mechanisms for stopping sabotage. In our current system we have assumed that the peers will be honest-but-curious when sharing their data, meaning that we have no way of detecting dishonest peers. In a real world system there would need to be safeguards against people which intend to either destroy the validity of the classifier created by feeding misinformation into the system, or people who tries to intercept and expose the data from other peers. Potential research ares would be intrusion detection in distributed systems, fraud detection, trust networks and reputation systems, and further research into encryption.  


\newpage
\listoftodos[Notes]

\cleardoublepage