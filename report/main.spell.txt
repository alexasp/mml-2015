
dedication (optional)

Summary

Write your summary here...

i

Preface

Write your preface here...

ii

Table of Contents

Summary i

Preface ii

Table of Contents iii

List of Tables v

List of Figures vii

Abbreviations viii

Symbols ix

1 Introduction 1

1.1 Objective and Scope / Problem Statement 1

1.2 Research Question/Goal 2

1.3 Thesis Structure 2

2 Background and Motivation 3

2.1 Concepts and Expressions 3

2.2 Privacy Breaches 4

2.2.1 Netix Prize Competition 4

2.2.2 Group Insurance Commission 5

2.2.3 New York Taxi dataset 5

2.3 Attack Vectors 6

2.3.1 Linkage Attacks [Rename] 6

2.3.2 Background Information 7

2.4 Challenges in Data Privacy 7

2.5 Motivation 7

2.5.1 Data security 7

2.5.2 Data ownership 7

iii

2.5.3 Future legal requirements 8

3 Basic Theory 9

3.1 Dierential Privacy 9

3.1.1 Denition of Dierential Privacy 10

3.1.2 Privacy Budget 10

3.1.3 Noise Mechanisms 11

3.2 Logistic Regression 12

3.2.1 Sensitivity of Logistic Regression Aggregation Mechanism 13

3.3 Ensemble learning 14

3.4 Cross validation 15

3.5 Programming Frameworks Used 15

3.5.1 JADE 15

3.6 Homomorphic Encryption 16

3.7 Resource consumption 17

4 Related Work 19

4.1 Dierential privacy 19

4.2 Centralized approaches 19

4.3 Distributed approaches 20

5 Experiment 23

5.1 Overview 23

5.1.1 Limitations of current implementation 24

5.2 Architecture 24

5.3 Dataset 25

5.3.1 Spambase 25

5.3.2 Australian Credit Approval 25

5.4 Parameter tuning 26

5.5 Validation 27

5.6 Algorithm 27

5.6.1 Platform Setup 27

5.6.2 Application of Aggregation Mechanism 27

5.6.3 Fitting Local Classiers 28

5.6.4 Propagation Of Published Models 28

5.6.5 Distribution 29

5.6.6 Experiment instantiation and reset 29

5.6.7 Communication 30

5.6.8 Learning 30

5.6.9 Privacy 30

5.6.10 Experiment 30

iv

6 Analysis 33

6.1 Baseline Classication results and comparisons 33

6.2 The importance of data 34

6.3 Importance of regularization 36

6.3.1 Analysis of Propagation and group size 37

6.3.2 Issues with cold start 39

6.4 Potential Future applications 41

6.4.1 Health 41

6.4.2 Private sharing of business data 42

7 Conclusion 43

7.1 Threats To Validity 43

7.1.1 Platform 43

7.1.2 Resource Consumption 43

7.1.3 Homomorphic encryption 44

7.2 Future Work 44

Bibliography 49

Appendix 49

v

vi

List of Tables

2.1 Table of basic categories of database attributes 3

2.2 Table of anonymization operations (adapted from ? ) 4

vii

viii

List of Figures

3.1 JADE Architechture 16

5.1 One iteration of model aggregation 30

6.1  = [10 103];  = 2 , 50 peers, 1 aggregation 34

6.2  = [0:3];  = 2 , 30 peers, 1 aggregation 35

6.3  = [0:1; 2:2];  = 2 , 50 peers, 1 aggregation 36

6.4  = 210;  = [2 24] , 50 peers, 25 aggregations, publish to
participants 36

6.5  = 0:1;  = [2 24] 50 peers, 25 aggregations, publish to all 36

6.6  = 1:0;  = [2 24] 50 peers, 25 aggregations, publish to
participants 38

6.7  = 1:0;  = [2 24] 50 peers, 25 aggregations, publish to all 38

6.8 Spambase.  = 1:0;  = [1:0] , 50 peers, aggregation sizes in range
[2, 50], 66 samples per peer 38

6.9 Spambase.  = 1:0;  = [1:0] , 50 peers, aggregation sizes in range
[2, 50], 20 samples per peer 40

ix

Abbreviations

AMS = Agent Management System DF = Directory Facilitator IMDB = Internet
Movie Database JADE = Java Agent framework for Distance learning
Environments PII = Personally Identiable Information PINQ = Privacy
Integrated Queries PPDM = Privacy-Preserving Data Mining PPDP =
Privacy-Preserving Data Publishing QID = Quasi Identier

x

Symbols

 = Privacy parameter that represents the privacy budget A = Parameter
which represents the privacy level of our aggregation mechanism  =
Regularization parameter  = Learning rate parameter A = Represents our
aggregation mechanism D = Represents a dataset M = Represents a general
privacy mechanism, like the Laplacian or the Exponential.

xi

xii

Chapter 1

Introduction

All over the world people are interacting with technology more than
ever; when using their cell phone, shopping online, visiting a doctor
who uses electronic records, and in countless other acts. This usage
generates a massive amount of information, leading to data being more
deeply integrated into our daily lives than ever before. Sintef
published a report in 2013 which stated that: "A full 90% of all the
data in the world has been generated over the last two years ?." With
this massive inux of information, new elds of both academic study and
commercial interest have appeared to nd out how to best analyze this
data.

The terms "big data" and "analytics" have been widely used as common
designations for this emerging eld of technology. The communal denition
for describing big data stems from a 2001 research report?, in which
analyst Doug Laney dened the problem of being a three-dimensional
challenge: "“Big data is high-volume, -velocity and -variety information
assets that demand cost-eective, innovative forms of information
processing for enhanced insight and decision making." The rst part of
his challenge, commonly known as the 3 Vs of big data, deals with the
necessary qualications for data to be called "big data", while part two
and three is the how and why.

The wide variety of the potential applications of big data analytics
have also raised essential questions about whether our social and
ethical norms are sucient to protect privacy in a world which has
entered "the era of big data". Both in the European Union and in the
United States there have been eorts made to create new laws for handling
data privacy. The Council to the President, an advisory group to the US
President, concluded in their 2014 report ? that preserving privacy
values would be their number one recommendation when designing a new
policy framework for big data. Furthermore, they advised that more than
70 million USD should be made available to federal research in
privacy-enhancing technologies.

1.1 Objective and Scope / Problem Statement

The objective of this study is to contribute to the aforementioned eld
of study, more specically in the area of Privacy-Preserving Data Mining
(PPDM). In our work we

1

Chapter 1. Introduction

explore the feasibility of employing a privacy-preserving technique
called dierential

Right word?

Right word?

privacy. Relying on previous work in homomorphic encryption and peer to
peer communication, we would like to create an framework that allows for
distributed, scalable machine learning while preserving the privacy of
the participants.

R1: How big is the loss of accuracy in a distributed, dierentially
private system, compared to a centrally trained model?

While there have been research on both distributed and dierentially
private machine learning system, there have been very little research
done on a combination of both. Results from research on dierential
privacy indicate that there often is a trade-o between privacy and a
loss of accuracy. We want to study this trade-o in our distributed
approach and analyze which factors comes into play and how they can be
handled in a way that leads to an optimal result.

R2: How can the variance in accuracy between participants be minimized?

Our system architecture is based on a notion of independent peers which
collaborate to create aggregated logistic regression models which is
used for classication. Due to there not being one single centralized
classier, there will most likely be a variance in the accuracy of the
classiers each peer hold. We want to explore options on how to reduce
this variance, so that we can reduce the likelihood of one peer having a
well-performing classier while another produces poor classication
results.

1.2 Research Question/Goal

Can we do distributed machine learning while still provide a dierential
privacy guarantee?

1.3 Thesis Structure

Here there will a short summary of each chapter

2

Chapter 2

Background and Motivation

In this section we will rst explain some basic concepts and expressions
that are used in the privacy context such anonymization operations and
Personally Identiable Information(PII). Then we will have a look at
some classic examples of failure to preserve privacy when data
publishing and how these attacks motivated us to choose our topic for
this thesis.

2.1 Concepts and Expressions

In the most basic form of privacy-preserving data publishing (PPDP), the
data holder has a table of attributes from the following four
categories: Explicit Identier, Quasi Identier, Sensitive Attributes,
and Non-Sensitive Attributes ?. A summary of each category can be found
in Table 2.1 .

Attribute name

Denition

Example

Explicit Identier

Explicitly identies record owners

Government identity number (e.g SSN)

Quasi Identier(QID)

Potentially identies record owners

Birth date and gender

Sensitive Attributes

Sensitive information about a person

Income, disability status

Non-Sensitive Attributes

All other attributes

Favorite band

Table 2.1: Table of basic categories of database attributes

From these categories, it would be easy to think that Personally
Identiable Information (PII) would only be found in the rst attribute.
As we will see in the next section, this is not the case. Recent privacy
laws have dened PII in a much broader way. They account for the
possibility of deductive disclosure and do not lay down a list of
attributes that constitutes as PII. For example, the European Parliament
made a set of directives known

3

Chapter 2. Background and Motivation

as the Data Protection Directive, in which personal data is dened as:
"any information relating to an […] natural person […] who can be
identied, directly or indirectly, in particular by reference […] to one
or more factors specic to his physical, physiological, mental,
economic, cultural, or social identity"`?.

In order to remove any PII from a dataset, it needs to go through a
process called anonymization. This constitutes a series of
modications/manipulations of with the ultimate end goal of protecting
the privacy of the dataset's participants. Fung et al.? operates with a
number of ve basic operations which might be applied for this purpose.
These operations are shortly described in Table 2.2 .

Anonymization Operation

Denition

Generalization

Replaces the value with more general value, such as a mean value

Suppression

Replaces the value with a special value, indicating that the replaced
values are not disclosed

Anatomization

De-associates the relationship between the quasi-identier and sensitive
information

Permutation

Partitions a set of data records into groups and shues their sensitive
values

Perturbation

Replace the original value with a synthetic value that keep the
statistical characteristics

Table 2.2: Table of anonymization operations (adapted from ? )

2.2 Privacy Breaches

In the recent years there have been many failures in privacy preserving
data publishing. Many companies have been faced with a PR disaster after
releasing data about their customers thinking them being anonymized,
only to have people de-anonymize their data and breaching the privacy of
the datasets' participants. In this section we will have a look at some
of these privacy failures.

2.2.1 Netix Prize Competition

Netix, the world's largest online movie streaming website, decided in
2006 to crowdsource a new movie suggestion algorithm and oered a cash
prize of 1 million dollar for the most ecient algorithm. To help the
research, they released 100 million supposedly anonymized movie ratings
from their own database. In order to protect the privacy of their users,
Netix removed all user level information; such as name, username, age,
geographic location, browser used, etc. They also deliberately perturbed
"some of the rating data for some customers[...] in one or more of the
following ways: deleting ratings; inserting alternative ratings and
dates, and modifying random dates"?. The released data records included
an anonymized user ID, movie name, date of rating, and the user's rate
on a scale from 1 to 5.

4

2.2 Privacy Breaches

Two researchers from the University of Texas,Narayanan and Shmatikov?,
demonstrated that an adversary who knows only a little bit about an
individual subscriber can easily identify this subscriber's record in
the dataset. Using the publicly available dataset from the Internet
Movie Database (IMDB) as the source of background knowledge, they
matched certain subscribers with their Netix records, and uncovered
their apparent political preferences and other potentially sensitive
information. The paper also oered a formal mathematical treatment of how
a small amount of auxiliary knowledge about an individual can be used to
do a fairly reliable re-identication. In the case of the Netix
dataset, the authors ? found that with only 8 movie ratings, 99% of the
records could be uniquely identied. Furthermore, they proved that the
de-anonymization algorithm they employed is robust to discrepancies in
the rating and dates.

2.2.2 Group Insurance Commission

In 1997, Latanya Sweeney wrote a paper on how she had identied the
medical records of Massachussets governor William Weld based on publicly
available information from the database of Group Insurance Commission.
She achieved this analyzing data from a public voter list, and linked it
with patient-specic medical data through a combination of birth date,
zip code, and gender?. As these columns were similar in both databases,
their combination could be used to identify medical records that belong
to either one person, or a small group of people. Sweeney hypothesized
that 87% of the US population could be identied by having the
combination of the three aforementioned records. It's worth noting here
that this theory is not conclusive. A paper by Daniel Barth-Jones
suggests that the re-identication of Weld may have been a uke due to
his public gure, and that ordinary people risk of identication is much
lower?.

2.2.3 New York Taxi dataset

The New York City Taxi and Limousine Commission released a dataset in
2013 containing details about every taxi ride that year, including
pickup and drop-o times, location, fare, as well as anonymized (hashed)
versions of the taxi's license and medallion numbers. Vijay Pandurangan,
a researcher for Google, wrote a blog-post where he showed how he
exploited a vulnerability in the hashing-function to re-identify the
drivers. He then showed how this could be potentially used to calculate
any driver's personal income?.

Another researcher, called Anthony Tockar, wrote an article during his
internship at Neustar Research where he proved that the dataset also
contained an inherent privacy risk to the passengers which had been
riding New York Taxis. Even though there was no information in the
dataset on who had been riding the taxis, Tockar showed that by using
auxiliary information such as timestamped pictures, he could stalk
celebrities and gure out to where they were driving, and how much they
tipped the driver. He also used map data from Google Maps to create a
map of drop-o locations for people that had exited a late night visit
from gentleman's club and taken a cab home. He then used websites like
Spokeo and Facebook to nd the cab customer's ethnicity, relationship
status, court records, and even a prole picture?.

5

Chapter 2. Background and Motivation

2.3 Attack Vectors

2.3.1 Linkage Attacks [Rename]

In each of the examples in the previous section, the privacy breach was
achieved through an attack model called linkage attacks. These types of
attacks are characterized that they create a decision rule which link at
least one data entry in the anonymized dataset with public information
which contain individual identiers, given that the probability of these
two matching exceeds a selected condence threshold.

In the literature??, they broadly classify the attack models into two
categories: Record linkage and attribute linkage. In both these types of
attack, we need to assume that the attacker knows the QID of the victim.

Record Linkage[rename]

In the case of attribute linkage, some quasi-identier value QID
identies a small number of records in the original dataset, which is
called a group. If the victim's QID is the same, he or she is then
vulnerable to being linked to this much smaller number of records in the
group. With the help of some additional information, there is then a
chance that the attacker could uniquely identify the victim's records in
the group. This is what happened to governor William Weld as mentioned
in section 2.2.2 . Sweeney linked medical data with a voter list, which
both included the QID= <Zip,Birth date,Sex >. She then employed the
background knowledge that governor Weld was admitted to the hospital at
the certain date, which allowed her to uniquely identify him from the
small group of people that shared the same QID as him.

k-anonymity Sweeney? proposed a notion called k-anonymity in order to
try and prevent record linkage through QID. She dened that a table T
with a quasi-identier QIT would satisfy k-anonymity if and only if each
sequence of values T[QIT] appears with at least k occurrences in T[QIT]
. From that denition it appears that k-anonymity is designed to prevent
record linkage through hiding the record of the victim in a big group of
records with the same QID. This method has a weakness however, as an
attacker can still infer a victim's sensitive attribute, such as having
the attribute hasDisease=true, if most records in a group have similar
values on those sensitive values.

Attribute Linkage[rename]

The aforementioned weakness is an example of an attribute linkage
attack. An attacker might not be able to precisely identify the victim
through a record, but can still infer his or her sensitive values from
the published data. The attacker does this based on the set of sensitive
values associated to the group the victim belongs to.

To prevent this type of attack, Machanavajjhala et al? proposed an idea
based on diminishing the correlation between the QID attributes and the
sensitive values, which they called l-diversity. The method requires
each group with similar QID to have l distinct values for the sensitive
attributes.

6

2.4 Challenges in Data Privacy

2.3.2 Background Information

2.4 Challenges in Data Privacy

Several studies have been performed to assess which privacy risks exists
in elds such as mobile applications citations, health care data, and in
social networks, and all of them found deciencies in either the
collection or handling of individuals' data. A study run by the European
Data Protection Authorities (DPA) found that out of 1211 mobile
applications surveyed, 59% caused concern with respect to
pre-installation privacy communications, and that 31% requested
permissions exceeding what the surveyors would expect based on their
understanding of the applications’ functionality?.

The law might not necessarily be enough to suciently prevent the misuse
of personally sensitive information, such as patient's health care data.
A study performed by Yale's center for bioethics concluded that: "Law
likely cannot catch up with burgeoning data collection, data
aggregation, and data mining activities, nor with technological advance,
let alone adequately anticipate it." Yet the author also argued that
technological progress would lead to "Better alternatives to
identication and de-identication; means of tracking data; [...]
improved data security; and returning benet to data originators"?.

2.5 Motivation

We hope to show that a competitive solution can be created in a
distributed learning setting, which also can provide a privacy guarantee
for the people who supply the data required for learning. If we are
successful, our research can open an avenue of practical solutions where
the paradigm in data mining shifts from collecting data in massive
centralized databases, to a distributed approach where the data
producers also become data owners.

2.5.1 Data security

This project is motivated by the aforementioned challenges and breaches
of data privacy, and wish to contribute to the development of
privacy-preserving technology. In a world where massive amounts of
sensitive personal data are being collected, attacks on the individual's
privacy are becoming more and more of a threat.

2.5.2 Data ownership

Addtionally, we are strongly motivated by the idea that there should be
a reversal in data ownership. Currently, companies oering services to
users collect the data stream generated by a user and store it centrally
in a data center owned by the company. The user has to trust that these
data centers will not be breached or leaked. Furthermore, the user has
to trust that the company policies or ethical standards will not change
in the future and that the company or their data will not be bought by a
independent third party. If data streams were instead collected in some
user-controlled repository, risk of breaches would be reduced and the
user would maintain full access control and monitoring. Tim Berners-Lee
voiced his support for this idea at the IP EXPO in 2014?: "I would like
us to build a world

7

Chapter 2. Background and Motivation

in which I have control of my data. I can sell it to you and we can
negotiate a price, but more importantly I will have legal ownership of
all the data about me,". He also brought up another compelling reason to
ensure that users retain the data they produce?: "In general … if you
put together all that data, from my wearable, my house, from other
companies like the credit card company and the banks, from all the
social networks, I can give my computer a good view of my life, and I
can use that. That information is more valuable to me than it is to the
cloud." A user will have multiple applications that gather information
from their daily life, such as exercise, social and oce applications.
While each of these data streams on their own can be useful for the
companies that collect them, they can have even more powerful uses when
put together to give a more complete context. Instead of each company
pulling user data to their data centers, users could push data stream to
their personal storage, and oer. The user then has control of who
accesses the data and how, while also allowing for data analysis across
completely separate applications.

2.5.3 Future legal requirements

The European Parliament is working towards new legislation that will
create a set of common data protection rules for all EU member states?.
This legislation oers right to erasure and right to portability. The
matter of portability is a step in the direction of the ideas of data
ownership discussed in Section 2.5.2 . Perhaps most signicantly, the
regulation requires that all companies operating from the EU or having
customers in the EU will be required to comply with. Companies that do
not comply can risk being imposed periodic data protection audits or
nes up to €100 million or 5% of annual worldwide turnover.

The European Parliament is not alone in looking new laws to regulate
data privacys. The Council to the President, an advisory group to the US
President, concluded in their 2014 report ? that preserving privacy
values would be their number one recommendation when designing a new
policy framework for big data. Furthermore, the so called "Privacy Bill
of Rights" outlined by the Obama administration in 2012 is moving
forward, and a new discussion draft was published in 2015?. Among the
requirements put forward in this bill is transparency about how data is
used, the degree of control a person has over how their data is used.

8

Chapter 3

Basic Theory

Common to all the attack vectors described in the previous chapter is
that the attacker rely on background knowledge, often also called
auxiliary information, to perform their linkage attacks. Protecting a
database against this threat has long been a major challenge in database
design. Already back in 1977 Tore Dalenius ? dened a desideratum for
data privacy which says that:

Access to the published data should not enable the adversary to learn
anything extra about target victim compared to no access to the
database, even with the presence of any adversary’s background knowledge
obtained from other sources.

This privacy goal was rejected by Cynthia Dwork, who showed the general
impossibility of Dalenius' goal due to the existence of auxiliary
information. Instead she chose to formulate a probabilistic privacy
goal, which places an upper bound on how much the risk of privacy breach
can increase by participating in a database.

3.1 Dierential Privacy

The term "dierential privacy" was dened by Dwork as a description of a
promise, made by a data holder to a data subject: "You will not be
aected, adversely or otherwise, by allowing your data to be used in any
study or analysis, no matter what other studies, data sets, or
information sources, are available." ? In an ideal situation, databases
which implement dierential privacy mechanisms can make condential data
widely available for accurate data analysis, without resorting to data
usage agreements, data protection plans, or restricted views.
Nevertheless, the Fundamental Law of Information Recovery states that
overly accurate answers to too many questions will destroy privacy in a
spectacular way ?, meaning that data utility will eventually be
consumed.

9

Chapter 3. Basic Theory

3.1.1 Denition of Dierential Privacy

The classic example for explaining a security breach is the case of Mr
White: Suppose you have access to a database that allows you to compute
the income of all residents in a specied area. If you knew that Mr
White was going to move, simply querying the database before and after
his relocation would allow you to deduce his income.

Denition 1. The distance of two datasets, d(D1; D2) , denotes the
minimum number of sample changes that are required to change D1 into D2
.

Formally, dierential privacy is dened as follows: A randomized function
M gives  -dierential privacy if for all data sets D1 and D2 where
d(D1; D2) = 1 , and all S  Range(M) ,

That is, the presence or absence of a particular record should not aect
the probability of any given output of M(D) by more than some
multiplicative factor.

Informally, the presence or absence of a single record in a database
should not have a noticeable impact on the output of any queries sent to
it. Though the existence of the database itself might allow attackers to
learn information about a person, opting out of the database will not
signicantly help reduce the risk of information disclosure. Conversely,
participating in the database does not signicantly increase the risk of
disclosure either, thus fullling Dworks promise quoted in the beginning
of Section 3.1 .

Privacy preserving data analysis platforms such as PINQ?, Airavat? and
Fuzz? have all implemented features such as privacy budgeting and noise
mechanisms to compute useful queries while fullling Equation 3.1 .

3.1.2 Privacy Budget

The quotient Pr[M(D1)2S] Pr[M(D2)2S] measures the extent to which an
attacker can ascertain the dierence between the two datasets?. Sarathy
and Muralidhar? calls this ratio the "knowledge gain ratio". Dierential
privacy requires that this ratio is limited to e . This is because as
the ratio grows larger, an attacker can determine with greater
probability that the query result was obtained from one dataset over the
other.

Privacy budgeting was introduced to limit the amount of information a
data analyst can obtain about any individual with data records in the
dataset. The data analysis platform will track every query to ensure
that both individual queries and aggregation queries do not exceed the
given budget. This privacy standard forbids further queries to the
database once the budget has been consumed.

Dening and depleting a privacy budget is possible due to the sequential
composition property of  -dierentially private mechanisms, as shown by
McSherry?. Given N mechanisms Mi that oer  N -dierential privacy,
applying each mechanism Mi in sequence oers  -dierentially privacy.

10

3.1 Dierential Privacy

3.1.3 Noise Mechanisms

Given a target function f to compute on a database D , it is necessary
to design a randomized function M which fullls Equation 3.1 while
yielding a useful approximation to the true f . This randomized function
M can be created by adding noise to the computation of f . There are
many dierent mechanisms for applying this noise, but the two most common
are the Laplace mechanism and the Exponential mechanism.

Laplace Mechanism

The Laplace mechanism involves adding random noise which follows the
Laplace statistical distribution. The Laplace distribution centered
around zero has only one parameter, its scale b , and this is
proportional to its standard deviation.

When using the Laplace mechanism it is necessary to choose a suitable
value for the parameter b . Increasing values of b results in increased
noise variance. The scale of b is naturally dependent on the privacy
parameter  , and also on the eect the presence or absence of a single
record can have on the output of function f . This risk is called the
sensitivity of the function, and is dened mathematically as:

Picture show- ing showing the laplacian distribu- tion distribution

Missing gure ]

Not sure if we need the pic. Alex: I think we do.

Not sure if we need the pic. Alex: I think we do.

This equation states that the sensitivity f is the maximum dierence in
the values that the function f may take on any pair of databases that
dier on only one row. Dwork proved that adding a noise drawn from
Lap(f=) to a query,  -dierential privacy? is guaranteed.

add a section explain- ing explaining that while it would seem that
sensitivity is would often be high, some functions like count are
naturally low sensitivity, and that steps such as clamping can be taken
to reduce sensitivity

add a section explain- ing explaining that while it would seem that
sensitivity is would often be high, some functions like count are
naturally low sensitivity, and that steps such as clamping can be taken
to reduce sensitivity

Exponential Mechanism

The exponential mechanism proposed by McSherry and Talwar ? is a method
for selecting one element from a set, and is commonly used if a
non-numeric value query is used. An example would be: "What is the most
common eye color in this room?". Here it would not make sense to perturb
the answer by adding noise drawn from the Laplace distribution. The

11

Chapter 3. Basic Theory

idea of the exponential mechanism is to select the output from all the
possible answers at random, with the probability of selecting a
particular output being higher for those outputs that are "closer" to
the true output.

More formally, let A be the range of of possible outputs for the query
function f . Also, let uf(D; a) be a utility function that measures how
good an output a 2 A is as an answer to the query function f given that
the input dataset is D (Note that higher values of uf represents better
outputs). The sensitivity function will then be dened as the maximum
possible change in the utility function's value uf due to the addition
or removal of one person's data from the input, i.e: Denition 4: the
sensitivity of score function uf is dened as

`

3.2 Logistic Regression

The logistic regression model is

where  is the parameter vector we wish to learn. It can be used for
predicting the probability of a binary outcome or binary classication
by setting a classication threshold. Given a training set we choose the
 with the largest likelihood, the maximum likelihood estimator (MLE)?.
The likelihood of parameter  is

For convenience, the point of maximum log-likelihood is used instead.
Since the log function is monotonically increasing, the maximum
likelihood estimator and the maximum log-likelihood estimator is the
same. We then select

where the term kk22 is a regularization term to restrict the magnitude
of  and avoid overtting the training data. Using  > 0 gives a
“regularized” estimate of w which often has superior generalization
performance, especially when the dimensionality is high (Nigam et al.,
1999).

We nd the point of maximum log-likelihood by gradient descent of the
regularized objective. The rule for updating each dimension j of 
becomes

12

3.2 Logistic Regression

Where  is the step length in gradient descent, also referred to as the
learning rate.

3.2.1 Sensitivity of Logistic Regression Aggregation Mechanism

In order to built logistic regression models in a way that preserves
privacy, it is necessary to determine the sensitivity of the output
model. Chaudhuri and Monteleoni? showed that the sensitivity of logistic
regression is at most

where n is the size of the training set and  is the regularization
parameter used in model training.

This solves only the case of training a privacy-preserving logistic
regression model on local data. Our research goal involves any number of
peers cooperating to build useful models without compromising the
privacy of their local data. Pathak, Rane and Raj? proposed an approach
where locally trained logistic regression classiers are aggregated by
averaging. Secrecy is achieved by using an encryption method to compute
the aggregate classier, ensuring that local data is not shared while
allowing a dierentially private model to be published. This encrypted
computation method is presented in more detail in Section 3.6 . It is
important to note that the approach of Pathak et al. assumes that the
participants are honest-but-curious. This assumption means that
participants will follow the established protocol, but will read any
information that is somehow available to it. Their method is not robust
against malicious sabotage.

When aggregating K locally trained models, their approach computes the
nal model

where  is a noise vector that guarantees  -dierential privacy. This
noise vector is drawn from the Laplace distribution with parameter 2
nj , where nj is the size of the smallest dataset used in training of
the K models. This means that they use a bound on output sensitivity of

which is the same as in Equation 3.9 , except that the lowest nj is
used. Since the lowest nj corresponds to the highest noise variance,
this gives protection to all the participants regardless of the size of
their dataset.

13

Chapter 3. Basic Theory

We can improve upon the sensitivity result by Pathak et al. By following
the same argument as in ?, we can show that the sensitivity of the
aggregated model is at most

where K is the number of participants.

Proof. As in ?, we consider a case where one record in data set D is
changed, giving the adjacent data set D0 . This data set is partitioned
to each of the participants P1 through PK Assume that the change is in
the dataset of participant Pj , then the change in the nal aggregated
model ^ws will only be in ^wj , the local model of Pj . We denoted this
changed model as ^w0j . Given the size of the smallest data set
partition n(1) , that there are K participants and that the sensitivity
of ^wj is at most 2 j , we have that

add term between rst and second

add term between rst and second

Prove or remove.

Prove or remove.

Show/explain that the same holds for the lower bound -epsilon also, as
in Pathak

Show/explain that the same holds for the lower bound -epsilon also, as
in Pathak

Add proof of above sensitivity

Add proof of above sensitivity

It is important to note that this does not oer full protection to
participants. It only oers dierential privacy guarantee for individual
records in their data set. This means that aggregate information about a
participants data set will go, and in principle sucient knowledge about
the other di6=k datasets would allow a third party to learn information.
For example, an individual might not want insurance companies to know
about the averages of features in their biometric records. The current
system would not help with that concern - it only protects against
specic knowledge about individual biometric records.

As stated by Dwork, groups of records can be protected ? Dwork points
out that

Explain how we can protect whole user sets and the balance be- tween
between data set sizes and limits

Explain how we can protect whole user sets and the balance be- tween
between data set sizes and limits

the end goal of dierential privacy is allow learning aggregate
information in a way that privacy.

3.3 Ensemble learning

In ensemble learning, the predictions of individually trained models are
combined to form a nal prediction?. In this project we will be using a
variant of ensemble learning called bootstrap aggregating or bagging, as
presented by Breiman?. Breiman showed that when changes in the training
set has a signicant eect on the trained model, bagging can give better
performance than training a single model on the learning set. Bootstrap
aggregating involves creating new learning sets by sampling from the
original set with replacement, and training a model on each new set
produced. These models are all added to the ensemble, which then makes
predictions by taking a majority vote.

14

3.4 Cross validation

We did not strictly use bagging according to its formal denition, as the
bootstrap step was not used. In our approach models are instead trained
on disjoint subsets of the training set, which are then published after
being aggregated according to Equation 3.10 . There can be many such
models published, so they are added to ensembles and prediction is done
in the same fashion as in bagging.

3.4 Cross validation

We initially divided the data sets into a training set and a testing
set, the latter being intended to evaluate the performance and
properties of our approach. We needed to explore many dierent
combinations and variations of our experiment during, but the test set
should only be used as a nal step. If the test set is used for repeated
validation of dierent parameters, we would risk overtting it and
getting unrealistic test results.

One way to do reliable accuracy estimation is with cross validation,
which makes more ecient use of training data than creating a separate
holdout set and has less bias than as shown by Kohavi ?. Cross
validation involves partitioning the training set into K disjoint sets.
Then, for each t 2 [1; K] partition t is used as the test set, and the
remaining partitions are combined to form the training set. Accuracy is
reported as number of correctly classied instances divided by the total
number of instances over all K partitions.

Kohavi recommends 10-fold stratied cross validation. Stratied cross
validation involves ensuring that each fold has the same class
distribution as the original data set. . Since the data sets we tested
with have thousands of records and close to uniform class distribution,
we concluded that stratied folds was not necessary. Our experiments
were evaluated with 10-fold cross validation with each fold being a
random, disjoint subset of the training set.

3.5 Programming Frameworks Used

3.5.1 JADE

To minimize the risk of errors we wanted to implement the experiment in
such a way that it was easy to reason about the behavior of the
components and identify mistakes. Since the core of our experiment
involves peers communicating and cooperating to create predictive
models, we decided an agent-based model was suitable.

The Java Agent framework for Distance learning Environments(JADE) is a
middleware which facilitates the development of multi-agent systems. An
application based on JADE is made of a set of components called agents,
where each one has an unique name. Agents execute tasks and interact by
exchanging messages between each other. Agents execute on top of a
platform that provides them with basic services such as message
delivery. A platform is composed of one or more containers, where the
containers can be executed on dierent hosts thus achieving a distributed
platform. The Main container is a special container which exists in the
platform, as it has two special properties. 1: It must be the rst
container to start in the platform, and all other containers must
register to it. 2: Two special agents are included; the Agent Management
System (AMS) which represents

15

Chapter 3. Basic Theory

the single authority in the platform, and is an agent tasked with
platform management actions such as starting and killing other agents.
The other special agent is the Directory Facilitator (DF), which
provides a directory which announces which agents are available on the
platform. This acts like a yellow pages service where agents can publish
the services they provide and nd other agents providing services they
need.

Note that while all containers in a single platform must register with
the Main container in that platform, multiple Jade platforms can be
instantiated separately and communicate with each other, allowing for
scalability of Jade deployments.

Figure 3.1: JADE Architechture

Consider remaking this gure so we don't have to cite it

Write how we adapted the use of JADE here

3.6 Homomorphic Encryption

Homomorphic encryption is an encryption scheme which allows computations
to be carried out on ciphertext, meaning plaintext that has been
encrypted using an algorithm and a public key. The result of the
computations is also encrypted, and can be deciphered back

16

3.7 Resource consumption

to plaintext using a private key. This has long been considered as
crypthography's holy grail ?, as this would allowing operating on
encrypted text without knowing the decryption key. For example, given
ciphertexts C1 = Enc(Data1) and C2 = Enc(Data2) , an additively
homomorphic encryption scheme would allow to combine C1 and C2 to obtain
EncK(Data1 + Data2) . More concretely this means that if you encrypt
your data using such an encryption scheme, you can transfer your data to
an untrusted server which can perform some arbitrary computations on
that data without being able to decrypt the data itself.

Up until recently, all published homomorphic encryption schemes only
supported one basic operation, most commonly addition. These schemes
could only be called partially homomorphic, as they did not provide any
extensive functionality. The notion of a fully homomorphic encryption
schemes was rst proposed by Rivest, Adleman, and Dertouzos in 1978 ?,
but it wasn't until 2009 that Craig Gentry published a doctoral thesis
where he proved that he had constructed a fully homomorphic scheme?.
Gentry's solution was based on "ideal lattices" as well as a method to
double-encrypt the data in such a way that the errors could be handled
"behind the scenes". By periodically unlocking the inner layer of
encryption underneath an outer layer of scrambling, the computer could
hide and recover from errors without ever analyzing the secret data.

Do we need to talk about a method of HE that doesn't really work for our
requirements?

Do we need to talk about a method of HE that doesn't really work for our
requirements?

The downside of Gentry's two-layered approach is that it requires a
massive computational eort. Bruce Schneier, a leading American
cryptographer, pointed out "Gentry estimates that performing a Google
search with encrypted keywords { a perfectly reasonable simple
application of this algorithm { would increase the amount of computing
time by about a trillion. Moore's law calculates that it would be 40
years before that homomorphic search would be as ecient as a search
today, and I think he's being optimistic with even this most simple of
examples?."

3.7 Resource consumption

Give analysis of time and space complexities and how much this taxes a
device.

Give analysis of time and space complexities and how much this taxes a
device.

17

Chapter 3. Basic Theory

18

Chapter 4

Related Work

In this chapter we will present some of the existing works related to
our thesis and research goals. The papers referenced in this chapter are
naturally also related to this thesis' constituent papers. We have
chosen to mention papers that are related to one or more of the major
theoretical framework we have employed. This chapter will therefore be
structured in three sections: the rst will give the main theoretical
contributions to the concept of dierential privacy. The second section
will explore some centralized approaches to machine learning with
dierential privacy guarantees. The third and last section will explore
works that have employed a distributed approach similar to our own
framework.

4.1 Dierential privacy

As mentioned in Chapter 3 , dierential privacy was dened by Cynthia
Dwork in her seminal work published in 2006 (?). This paper lay the
mathematical foundations for the privacy guarantee we employ in our
work. Dwork later expounded on her work in the book Algorithmic
Foundations of Dierential Privacy(?), which we used as a main piece of
reference when gathering knowledge in the early phases of this thesis
project.

Other dening works include the two papers written by Frank McSherry.
His work on mechanism design (?) allowed for the expansion of Dwork's
Laplacian mechanism design, by providing the theoretical analysis that
other mechanisms could satisfy the same guarantee (see section 3.1.3 for
more). His paper (?) on the design and implementation of PINQ were
hugely inuential in the early phases of our project, as we could use
the paper and the publicly available code when implementing our own
design.

4.2 Centralized approaches

In the years following the release of Dwork's seminal paper there have
been a steadily increasing amount of publications, reaching a peak in
2013 with 141 papers published and indexed by the Scopus scientic
database. Some of these works have focused on the same

19

Chapter 4. Related Work

area as us, namely classication using logistic regression, but instead
opted to focus on a centralized approach. Mentioned below are the work
which have either inuenced our own, or perfomed similar experiments.

Chaudhuri and Monteloni designed a logistic regression algorithm which
guaranteed dierential privacy in 2009. They also provided a mathematical
proof for the upper bound of the sensitivity of logistic regression (see
Section 3.2.1 ), which formed the basis of our own solution. The same
authors provided a follow-up paper(?), in which they further developed a
method called object perturbation to add noise to the regularized
objective function. Their results which showed that objective
perturbation is generally superior to output perturbation has proved
very useful to the eld of dierential privacy.

Zhang et al (?) further improved upon Chaudhuri's work by creating a new
functional mechanism for objective perturbation, which they tested on a
set of census data by employing both linear and logistic regression.

4.3 Distributed approaches

As our research goal states, we wish to create a framework to test the
feasibility of employing a distributed, dierential private learner. One
of the rst works in this eld was performed by Pathak et al (?), who
proposed a privacy-preserving protocol for composing

is this correct reference formatting?

is this correct reference formatting?

a dierentially private aggregate classier. Their protocol trained
classiers locally in dierent parties, and the parties would then
interact with an curator through a homomorphic encryption scheme to
create a perturbed aggregate classier. We took inspiration from their
protocol when we created our own ensemble classier, extending the work
of Pathak et al. in several ways. We've taken steps to ensure better
scalability by adding better group forming for each of the peers, and
we've added an publishing step to the aggregation mechanism which allows
for the creation of an ensemble classier in each peer. Lastly we've
also performed more extensive experiments to validate the employed
method, as Pathak et al. seemed to have focused more on the theoretical
side of the experimentation.

Since the work of Pathak et al was presented in 2010 there have been
some research published on how to create private distributed learners.
One such example is the work of Boutet et al.(?), who presented a
privacy-preserving distributed collaborative ltering scheme which
relied on user prole obfuscation and randomized response. Another
interesting paper is the work of Zhang et al (?), which investigate
mechanisms to sanitize location data used in recommendation system with
the help of dierential privacy.

Rajkumar and Agarwal (?) presented an alternative to Pathak's method in
2012. It works in a multiparty setting by using a stochastic gradient
descent based procedure to directly optimize the overall multiparty
objective rather than Pathak's method of combining classiers learned
from optimizing local objectives. Their algorithm achieves a slightly
weaker form of dierential privacy than that of Pathak et al., but is
more robust to the number of parties and the relative fractions of data
owned by the dierent parties.

Ji et al. (?) recently proposed a distributed solution using logistic
regression, which learned from both private and publicly available
medical datasets. Their solution dier from our own as they employ a
globally synchronized structure, whereas our own solution works
asynchronously. They also design a mechanism which rst uses public
datasets to compute the gradient without any form of noise addition, and
then perform a distributed

20

4.3 Distributed approaches

logistic regression step with dierential privacy.

21

Chapter 4. Related Work

22

Chapter 5

Experiment

5.1 Overview

As presented in Section 1.1 , we wanted to do test an architecture that
allows fully decentralized machine learning that maintains the privacy
of the participants.

We consider a setting with N peers that each have a local data set.
These data sets are assumed to be independently sampled for each peer,
but may be sampled from the same distribution. When the system
initializes, each peer trains a logistic regression model on its local
dataset. The data set and the trained model is private and should only
be known by its owner.

After the initialization phase, the aggregation phase begins. In this
phase, the aggregation mechanism described in Section 3.2.1 is applied
one or more times, using the private model held by each peer. The
mechanism is not applied to all peers at the same time. Instead, subsets
of peers are selected randomly to form aggregation groups, each group
producing a single aggregate model. Many such groups can be formed, and
the group size can vary from including all the available peers to
including just a single peer. In our experiments, we specify a constant
group size which is used until the end of the experiment.

While the output of each mechanism application is an average of the
input models, produced in way that guarantees dierential privacy, the
computation itself must be done in a central manner. This is possible to
do securely using the protocol detailed by Pathak et al., which uses
homomorphic encryption to compute the model aggregate without allowing
the any of the participants to know the original, private model of
another participant?. Since this protocol requires some central
computation, one of the peers is chosen at random to be the curator,
responsible for acting as the central party described in the solution by
Pathak et al. The other peers in the group will submit the necessary
information to this curator, including their private model, in an
encrypted fashion. Once the peer acting as curator has received a model
from all participants, it computes the average model, adds noise sucient
to guarantee  -dierential privacy and publishes the nal result. The
target of this publish step can vary. In our experiments we have tested
one version that publishes a model to all available peers and one that
only publishes the model to the peers in the

23

Chapter 5. Experiment

group that helped create it.

Each peer holds a privacy budget, as discussed in Section 3.1.2 , that
limits how many times it can be involved in a mechanism application. If
the budget of a peer is depleted, it is no longer a candidate for the
randomly formed aggregation groups. When there no longer is enough peers
to form a group with the size specied for a particular experiment, the
experiment terminates.

5.1.1 Limitations of current implementation

Certain parts of this system is not implemented in this project, and are
replaced by black-box substitutes that simulate the required behavior.
We have only done this for components that are already described and
tested in other work.

The protocol created by Pathak et al. for computing aggregates securely
was not implemented. In our implementation models are sent unencrypted
to the peer acting as curator. While this part would need to be replaced
with a full implementation of the approach by Pathak et al. the output
returned by the curator is exactly the same, using the computation in
Equation 3.10 .

Finally, the selection of random groups is done in a non-scalable
manner. A centralized actor that has full knowledge of all participating
peers randomly selects groups of these peers and sends a message to each
peer with the list of participants. This should be replaced by a
decentralized method for the system to be scalable. How we intend to do
this is discussed further in Section 7.2 on Future Work.

5.2 Architecture

We designed a distributed system using the JADE framework. The core
component in this system is a PeerAgent, which represents a participant
in the distributed learning setting. This agent contains what would be
the local data of a person using some application. In the remaining
sections, whenever we say "peer" we are refering to the PeerAgent
described here, holding a local data set and with means of communcating
with other PeerAgent instances.

To form aggregate models it is necessary to select groups of peers to
create each model. In our experiment, this is implemented with a
singleton agent we named the GroupAgent. This agent draws random subset
of size g from the set of all peers. The size and number of groups
formed is given by parameters selected at the beginning of the
experiment, which are described in detail in Section 5.4 .

As stated in Section 5.1 , the experiment terminates when there no
longer a sucient amount of active peers to form a group. That is, when
the number of peers with sucient budget is less than the group size
parameter, the experiment is stopped. This behavior was implemented into
an agent we named CompletionAgent. This agent listens to messages from
the curators in the dierent groups. Once it has received messages of the
expected number of aggregated models having been created, it initiates
the nal step of the experiment, including testing performance metrics
and preparing the JADE environment for the next experiment.

24

5.3 Dataset

5.3 Dataset

This section will introduce the dataset(s) used. What features it
contains, what we try to learn/classify, and why we chose to use it.

5.3.1 Spambase

The Spambase dataset ? was used as a baseline training set. This dataset
is publicly available from the UCI machine learning directory, and
contains 57 input attributes of continuous format which serves as input
features for spam detection and 1 target attribute in discrete format
which represents the class.

We chose this dataset as it is a popular dataset to analyze the
performance of binary classiers, so that we could compare the results
of other logistic regression classiers against our own. While this
dataset might not seem like the ideal choice for testing a dierentially
private classier due to its lack of personal information, we argue that
it still ts well for the purpose of demonstration. In a
spam-classifying system based on our distributed model, a logistic
regression model can be built by training it locally in each user's
personal mail folder and then aggregated into an ensemble. That way you
can build a diverse spam-classier without the users having to give up
their personal email to a centralized database.

Before we could use the dataset, we needed to use normalization to scale
the data to 0-1 range. This is due to the proof in Chaudhuri paper which
states the assumption jXij < 1 . The scaling was based on the formula

We appended a feature with constant value 1.0 to all data records, to
act as the intercept or bias term.

5.3.2 Australian Credit Approval

Another dataset we used, was the Australian Credit Approval (Statlog),
which concerns the approval or disapproval of credit card applications.
It is publicly available from the UCI machine learning directory. This
is a much smaller set of data, with only 690 samples spread over 14
attributes. The data is useful for binary classication as it contains a
good mix of attributes: continuous, nominal with small numbers of
values, and nominal with larger numbers of values.

We mostly used this dataset to conrm and double-check the conclusions
we drew from the Spambase dataset, as it contains too few data records
for it to be 100% ideal for our use. Due to the sparsity of data we had
to scale down the amount of peers in each experiment, so that each peer
could get enough data to create a decent local classier. We chose to
keep on using this data as one of the motivating examples in Dwork's
book? of dierential privacy, is to protect data holders from insurance
and credit card companies.

This dataset was preprocessed in the same manner as was described in
Section 5.3.1 .

25

Chapter 5. Experiment

5.4 Parameter tuning

Number of peers P species how many dierent peers participate in the
experiment, and necessarily the number of partitions of the training
data sets. The training is divided into P parts of equal size.

Rationalize why we have included 1 in the 10-inteval experiments - it is
because 1 is a very interesting edge case. Should also talk about the
signicance of group size 1 in analysis.

Aggregate models are created from local models at each peer through an
aggregation process that is performed one or more times with subsets of
peers. The parameter g species how many peers will participate in a
single model aggregation. Since each peer has a unique subset of data,
this parameter determines how many partitions of the training set
contribute to the published aggregate models. These data partitions do
not contribute directly, but indirectly through the aggregation of
models trained locally on each partition.

Each peer trains a local logistic regression classier on its data
partition. This requires selection of a learning rate  , a
regularization constant  and a maximum number of iterations of gradient
descent I . The learning rate is sensitive to the size of the local
training set?, and should be tuned individually by each peer. We did
this by running 3-fold cross validation when each peer ts its local
model to identify the best  in the range [ 0] . 3-fold cross validation
was chosen because of both project computer time constraints and
experimental data constraints. Each experiment in its entirety is tested
with 10-fold cross validation, so it was necessary to reduce local model
training time in order to run in a reasonably short time on a single
computer. The data constraints is a part of the domain we want to
explore. When the amount of data is very small, 3-fold cross validation
oers a balance between parameter search reliability and validation set
sizes.

In usual data mining applications the regularization  would be tuned in
this manner as well, but the sensitivity of the aggregation mechanism
depends on  , as seen in Equation 3.11 . This means that the peers will
have to communicate to either agree on a regularization level or to
determine the smallest regularization constant to identify the worst
case noise level. In our experiments we chose a global regularization
level, which was used by all peers. We identied the best  by testing a
coarse grid of powers of 2 whenever we changed the per-peer number of
training samples.

Explain the specic way of how regularization was establish. Probably:
whenever number of peers changed, we ran an experiment over
regularization with other parame- ters parameters xed to constant
values. Explain why this is okay. We should also discuss possible
problems with this.

Explain why we are doing the inital test with a range of regularization.
Essentially, it is because epsilon can be seen as xed, learning rate is
found by CV and lambda is the only remaining parameter that is
essentialy to tuning the performance of individual models.]

Talk about the problem of tuning regulariza- tion. regularization. We
are essentially doing a global selec- tion selection of regularization.
This could be dicult. Better to communicate the minimum, perhaps? But
then, which regu- larization regularization should peers pick locally?
The high- est highest one that stil has good performance?

Talk about the problem of tuning regulariza- tion. regularization. We
are essentially doing a global selec- tion selection of regularization.
This could be dicult. Better to communicate the minimum, perhaps? But
then, which regu- larization regularization should peers pick locally?
The high- est highest one that stil has good performance?

The privacy parameter  determines the level of privacy for each data
partition. Note that this parameter does not apply to the original
training set as a whole - each peer has its own private database, which
is protected by  -dierential privacy.

Finally, the parameter  can be divided across several applications of
the aggregation mechanism, as described in Section 3.1.2 . This was
achieved with a per-aggregation parameter i . Each data partition can
participate the aggregation mechanism n times, where nA   .

Insert logistic regres- sion regression training algorithm including
hyperparame- ter hyperparameter usage

Insert logistic regres- sion regression training algorithm including
hyperparame- ter hyperparameter usage

26

5.5 Validation

5.5 Validation

The test sets set aside could not be used when tuning and evaluating
system hyperparameters. In order to explore the eects of the various
hyperparameters we used cross validation with number of folds n = 10 .
For a given combination of hyperparameters, performance metrics were
measured as their average across ten repetitions. In repetition i , data
fold i was used as validation set and the remaining n  1 data folds were
combined to form the test set.

For the XXXX experiment , the test set was used.

insert experiment here

insert experiment here

5.6 Algorithm

This section explain the logistic regression algorithm, how it is
commonly used, and what modications are needed when used in a
distributed setting. Explanation on how it is used in a dierentially
private manner is explained in the architecture section.

5.6.1 Platform Setup

5.6.2 Application of Aggregation Mechanism

The central element in our experiments is the aggregation mechanism A ,
which takes a set of models. This mechanism is given in Algorithm 1 . As
presented in Section 3.2.1 , the sensitivity of logistic regression
depends on the sizes of the data sets used to train the models.
Specically, the mechanism needs to know the size of the smallest
training set in order to guarantee dierential privacy. It is important
to note that the method we are

27

Chapter 5. Experiment

testing assumes honest-but-curious participants, as assumed by Pathak et
al?.

Input: Input:  - privacy parameter M - set of models trained by
participating peers;

N - set of peer training set sizes;

 - regularization level used when training each model in M ;

Output: Output: Perturbed aggregate of the models in M

nmin   min(N) ;

   Laplace(0; 2 nmin) nmin) ;

modelagg   1=K PjNjj=1 PjNjj=1 wj wj +  ;

return modelagg

Algorithm 1:  -dierentially private aggregation mechanism

Input: Input: P - the set of peers;

 - privacy parameter;

A - privacy level of a mechanism application;

A - the A -dierentially private aggregation mechanism;

group size - number of peers in a single mechanism application

for peer 2 P do

budgetpeer    ;

end

while jPj  group size do

group   randomSample(P; group size) ;

modelagg   A(group) ;

for peer 2 group do

budgetpeer   budgetpeer   A ;

if budgetpeer < A then

P   P r peer ;

end

end

publish(P; modelagg)

end

Algorithm 2: Application of aggregation mechanism

Defend randomSample procedure using Newscast reference

5.6.3 Fitting Local Classiers

5.6.4 Propagation Of Published Models

Originally in our system, aggregated models were only propagated to the
peers that had participated in creating that model, as can be seen in
Figure 5.1 . What resulted from this, especially when epsilon was set to
a low amount such as 0.1 or lower, was that the high amount of noise
made the classiers have a big standard deviation on their mean
classication rate. What this meant was that while the classiers could
be very accurate in some peers, classifying up towards 90% accuracy, it
could also be signicantly worse in other peers.

We theorized that we could improve the ensemble classier in each peer
if we could

28

5.6 Algorithm

propagate the aggregated models to all the peers in the network, instead
of just those who had participated in making them. Our hypotheses was
that this would lead to more stable classiers with lower standard
deviation, due to a smoothing eect in having more models in the ensemble
classier in each peer. This is basically the same idea as bootstrap
aggregating, or bagging, which has been proven to lead to improvements
in unstable procedures?.

Denitely talk more about the bagging ef- fect, eect, either here or in
the analysis section

Denitely talk more about the bagging ef- fect, eect, either here or in
the analysis section

For this reason, we decided to run experiments to compare the dierent
possible model. In all cases, the published models will have been
perturbed with Laplacian noise to give  -dierential privacy. In the
group publication setting, only the peers that join together to produce
a perturbed model will receive the nal result. In the full publication
setting, all peers active in the network will receive all perturbed
models.

Note that there is no selection or pruning of the ensemble classier
owned by each peer. If a peer receives a model, it will blindly add it
to the ensemble. This means each peers ensemble model will grow much
faster in the full publishing setting, and they will all contain
essentially the same models, the only exception being the unperturbed
model produced by the peer locally. We anticipated that this would lead
to a reduction in ensemble model accuracy variance.

5.6.5 Distribution

Introduce notion of distributed machine learning.

Why did we choose to perform distributed learning?

How does it t with the notion of dierential privacy?

Can we guarantee dierential privacy while doing it distributed

Record-based dierential privacy. Does it work?

This section should maybe go somewhere else, but where?

Figure explain- ing explaining our frame- work framework

Missing gure

5.6.6 Experiment instantiation and reset

Each combination of parameters described in Section 5.4 is run with
10-fold cross validation. Cross

29

Chapter 5. Experiment

5.6.7 Communication

Peer

How is each peer set up, and what behaviors do they implement? How do
they update then

Model training on agent instantiation.

Model training on agent instantiation.

propagate the model being learned. How do they know when to stop?

messaging

How do the peers communicate with each other? What does a message look
like? What is the PeerGraph? What controls the messages and determines
where they should go?

Figure 5.1: One iteration of model aggregation

5.6.8 Learning

How is a logistic model implemented in our framework?

How is a model created and passed around the network? Each Peer creates
a logistic model based on their local data. They then form groups by
calling the GroupFormingManager which assigns a group of peers together.
This groups creates an aggregated model based on their own local ones.
Here we simulate a Homomorphic Encryption scheme which assigns noise by
secret sharing.

How does the ensemble learning choose the best model? What kind of
performance metrics are used? Mean classication error and Confusion
Matrix.

5.6.9 Privacy

How does our framework guarantee dierential privacy? Based on the paper
by

5.6.10 Experiment

How are the experiments set up? Explain the testing scheme.

Jade is re-run with while changing the initial parameters for the amount
of peers, and the size of the groups they form. This test if performed
over 10 iterations, while the mean

30

5.6 Algorithm

classication error is recorded for each iteration. Each peer also
create a confusion matrix with their classication results, and the peer
with the best classication accuracy is saved and used to create a
ROC-curve.

Automatic testing scheme - Jade is congured so that it resets the main
container after each experiment, and then re-run with new parameter
congurations.

Implemented 10-fold cross validation. This way we can tune the
parameters and not be scared of our model overtting on the test data.
our initial solution was to use a single training and a single test set.
This way we tuned the parameters so that they would give the best
possible accuracy on the test set, which is not how the system should
behave in the real world.

31

Chapter 5. Experiment

32

Chapter 6

Analysis

Make a table with baseline classication rate for logistic regression,
put in our values. 1 Peer with no noise, 1 peer with noise, best case
from other experiments that we want to highlight. This can lead to a
discussion of why our results are not as good, where they show promise
etc

6.1 Baseline Classication results and comparisons

Error Rate/Spambase Sharma? 0.0705 Kumar? 0.1389 Our optimal result ( 
=10, 1 peer) 0.130 Less noise, less peers(  =0.1, 10 peers) 0.138 Less
noise, more peers(  =1.0, 100 peers) 0.164 Noisy result (  =0.1, 50
peers) 0.220

All prediction results given in this section are presented with mean and
standard deviation values. These values are computed by evaluating each
combination of parameters with 10-fold cross validation and taking the
mean and standard deviation of accuracy across the 10 data folds.

As explained in section 3.1 , dierential privacy works by disguising an
individual's data in a dataset by adding noise to their records. The
amount of noise added is determined by the privacy parameter  , and
grows exponentially the closer the parameter gets to zero.

33

Chapter 6. Analysis

Figure 6.1:  = [10 [10 103];  = 2 2 , 50 peers, 1 aggregation

Figure 6.1 shows the eect of the privacy parameter  in our experiment.
We wanted to test the eect of varying the  -value in the range from 2
to 29 , especially to nd out how the classier would perform when faced
with data with high amount of noise added to it. The positive class rate
in the UCI Spambase dataset is 0.4, so any error rate at this level is
no better than a random classier. The plot shows how sensitive output
is to the values of  .

Figure 6.3

6.2 The importance of data

One of the more important ndings for Data is important, the more data a
peer have, the greater the chance is that it will make a decent local
classier. J

34

6.2 The importance of data

Figure 6.2:  = [0:3];  = 2 2 , 30 peers, 1 aggregation

Figure 6.2 indicate how the classication performance improves as the
amount of data records available to each peer grows. When each peer only
have a small amount of data to create their local classier, the
classier tends to have display terrible performance. As peers gain more
data, both the performance and the variance of the classier improves.

The reason for this improvement is two-fold. 1: A bigger sample size for
the logistic regression model generally leads to better performance ?.
2: The sensitivity of logistic regression (see equation 3.9 ) is bounded
by the size of the training set. What this means is that the more data a
peer have available, the less amount of noise is needed to obfuscate
their logistic model.

The observation we've made is therefore thoroughly grounded in theory,
and is important to highlight when discussing our main research
question. Until a certain amount of data has been gathered, a system
based on our distributed architecture will display very poor
performance. What is interesting however, is that the amount of data
needed seems to be a relatively low number. In the paper written by
Pathtak et al?, they report that each party was given at least 3256 data
records. In our experiments we found that a much smaller amount of data
could still be used and create decent classiers. We believe this is due
to our choice of propagating every model out, so that each peer can
create an ensemble of classiers.

Fix this last sentence. It is not that good

35

Chapter 6. Analysis

6.3 Importance of regularization

Figure 6.3:  = [0:1; 2:2];  = 2 2 , 50 peers, 1 aggregation

Figure 6.4:  = 210;  = [2 [2 [2 24] , 50 peers, 25 aggregations,
publish to participants

Figure 6.5:  = 0:1;  = [2 [2 [2 24] 50 peers, 25 aggregations, publish
to all

Figure 6.5 shows the normal eect regularization has on accuracy for the
spambase data set, by setting  so high that noise is essentially
nonexistent. As the regularization

36

6.3 Importance of regularization

parameter  grows large, the model becomes less able to t the training
data, eventually resulting in models predicting only the negative class,
which constitutes 60% of the data set. This happen because the high
regularization forces the parameter vector to the zero vector, resulting
in uniform class probability for all samples.

On this particular dataset it appears that a logistic regression model
is not at risk of overtting, since the cross validated error does not
increase when the level of regularization is very low. Ignoring the
eects of privacy mechanisms, this would mean that selecting some
regularization parameter in the range [10 10 could be acceptable.
Choosing a level at the high end of this range could be a good idea, to
reduce the risk of overtting

Figure 6.4 shows mean accuracy over a range of  similar to Figure 6.5 ,
where  is set to a level where the noise variance still has an eect on
prediction accuracy, as seen in gure 6.1 .

When noise with signicant variance is added to the model creation
process, tuning  will adjust noise variance as well. Equation 3.11
states that the noise variance is inversely proportional to  . The
choice of regularization then must balance the model exibility at lower
levels of  with the decreased noise at higher levels of lambda.

Talk about how it is interesting that regularization now must respect
something other than just model performance, and oer some guidelines on
how regularization should be considered, especially in a setting like
ours, with many independent parties

6.3.1 Analysis of Propagation and group size

As mentioned in Section 5.6.4 , we hypothesized that we could improve
the overall classication accuracy of our system by publishing the
aggregated models generated in phase 2. In this section we present the
eects of publishing method. Figure 6.6 shows the case

Rewrite this if we don't use phases

Rewrite this if we don't use phases

where each group of peers only share the perturbed, aggregate model
among themselves, while Figure 6.7 shows the results the model is sent
to all existing peers.

The most obvious eect of globally publishing models is that the standard
deviation is much lower than the group publishing case. This is not
surprising.

The truth is probably somewhere in the middle - we can't expect to
easily publish models globally in all settings. Additionally, there
might be situations were global publishing could be detrimental to real
world performance - for instance, if there are strong geographical,
temporal or demographic trends, it might be better to limit the amount
of model sharing to suitable subsets.

As the previous experiment indicates that publishing newly made models
to as many peers as possible is better , we performed an additional
experiment to determine whether

Add a reection talking about why this makes sense.

Add a reection talking about why this makes sense.

it in such a scenario would be better to perform many aggregations with
fewer models included in each aggregate or performing few aggregations
including many models. This was achieved by testing performance with a
range of values. In the experiment, each peer can only participate in a
single aggregation before reaching the limit set by the privacy
guarantee. For example, given a set of 50 peers, a aggregation size of
25 can only publish two aggregated models.

The results of this experiment is seen in Figure 6.8 . It is clear that
a smaller group size and consequently more aggregated models published
resulted in a strong reduction in accuracy variance. While the variance
is too high at larger group sizes to know much about the actual mean
value after only 10 repetitions of the experiment, it is unlikely that
it is smaller than the mean accuracy observed when the group size is
one.

37

Chapter 6. Analysis

Figure 6.6:  = 1:0;  = [2 [2 [2 24] 50 peers, 25 aggregations, publish
to participants

Figure 6.7:  = 1:0;  = [2 [2 [2 24] 50 peers, 25 aggregations, publish
to all

Figure 6.8: Spambase.  = 1:0;  = [1:0] , 50 peers, aggregation sizes
in range [2, 50], 66 samples per peer

A possible explanation of this result is that the eects of boosting
counters the loss in accuracy that results from the addition of noise.
Since models will have noise added to them before being published,
expending all data to produce a single model might yield a worse
classier than partitioning data, adding noise to each separate, weaker
model and

38

6.3 Importance of regularization

combining them in an ensemble. On the other hand, this eect could be
caused by some leakage of privacy which becomes visible over repeated
applications of the aggregation mechanism.

Figure out if there is some way we could test or prove that this is not
the case.

Figure out if there is some way we could test or prove that this is not
the case.

The interesting observation that can be made from this is that the best
situation is when there is no aggregation. A group size of one results
in only a single model being contributed, and is equivalent to each peer
publishing its local model with noise. One possible reason for this
could be that there simply is no value in averaging models in the way
done in our experiments and by Pathak et al.?. Neither our experiment or
the experiment by Pathak et al. help distinguish between these two
possibilities. The experiment by Pathak et al. only demonstrate that
their method for creating aggregated models has comparable performance
to adding noise to a centrally computed model. Additionally, this is
only demonstrated with large data sets. In their experiment, the minimum
data set size for any participant is 3256. With data sets this large, it
is possible they would have gotten similar results by testing a model
produced by a single participant, without performing additional
aggregation. However, no experiment evaluating this possibility remains.
Their theoretical conclusions stand, but experimentally validating the
value of aggregation is necessary.

Thus the key question is whether or not aggregation is worth the
complexity of a

look at a couple of vectors trained with large subsets of data and see
if they are sim- ilar. similar.

look at a couple of vectors trained with large subsets of data and see
if they are sim- ilar. similar.

homomorphic encryption protocol. If similar performance can be achieved
solely by ensemble classiers of dierentially private models published
by each peer, one could skip the complexity and risk of relying on a
cryptographic protocol to maintain privacy.

Another possible explanation for this observation is that aggregation
might be useful, but not when the peers all have samples of data from
the exact same distribution. This is the case with the Spambase data set
used in the experiment in Figure 6.8 .

One way to answer question would be by nding a data set which has
subsets that are produced by distinct distributions, and partitioning
data by source distribution. Due to time constraints we did not have
time to locate and prepare data sets that t this requirement.

See if we have time to do this. We REALLY sho1uld, otherwise it will
very possibly give a lower grade.

See if we have time to do this. We REALLY sho1uld, otherwise it will
very possibly give a lower grade.

An alternative avenue for validating the value of doing aggregation
could be to reduce the amount of data given to each peer. If just one
peer has data sucient to train a good classier, it is sucient. For that
reason it makes sense to stage a situation were it is highly unlikely
that even a single peer gets a lucky subset of data. Figure 6.9
demonstrates such a case. Again, there is not clear indication that
averaging multiple models is better than simply publishing them
individually and using them in ensemble classiers.

redo this experiment above with regularization=0, as indicated by the
previous gures shown

6.3.2 Issues with cold start

Section ?? important motivation for this project was the idea that
application users should be able to retain ownership of their own data,
and only share it to a degree that they choose.

Explain what cold start issues is. Discuss how this is very prominent in
our sitaution, and the dierent ways we can deal with the cold start, and
pros and cons of those approaches.

Explain what cold start issues is. Discuss how this is very prominent in
our sitaution, and the dierent ways we can deal with the cold start, and
pros and cons of those approaches.

What are the results of our experiments?

What did we learn from the basic structure of creating our framework?

What diculties did we encounter?

What can we take away from our experiments?

What should have been done better?

What did we learn from tuning the dierent parameters?

39

Chapter 6. Analysis

Figure 6.9: Spambase.  = 1:0;  = [1:0] , 50 peers, aggregation sizes
in range [2, 50], 20 samples per peer

More data per peer leads to better classication. With small number of
peers, there are more records per peer. This generally leads to better
classication accuracy. Why is this?

Australian: Sweet spot for regularization at a 2e-3, 10 peers 5
groupsize

What was the eect of tuning the regularization parameter? High and low
regularization leads to more SD on the error rate, but there is a sweet
spot.

What was the eect of increasing the epsilon? It seemed like it was
better to keep the perUpdateBudget smaller than the epsilon, so that the
models are aggregated more. Did this lead to better accuracy? Why do
more aggregations lead to better classication than having a bigger
budget but only one aggregation.

Compare the results of our distributed logistic regression classier
with the tradional ones in literature. Sharma and Arora ? report getting
92.95% classication accuracy on the same data set. Kumar et al?.
reports 0.1389 error rate before ltering when using logistic regression
combined with a least squares regularization method. Our classier can
compare to these under certain circumstances, even after noise addition.

40

6.4 Potential Future applications

6.4 Potential Future applications

This section will discuss the potential application of a system based on
our distributed machine learner.

6.4.1 Health

A growing worldwide market is the sale and usage of wearable sensors,
such as environmental sensors, motion sensors, and health sensors. A IHS
report ? from 2014 estimates that the market for sensors in wearables
will expand to 135 million units in 2019, up from 50 million in 2013.
These wearables will evolve from being just a single purpose device such
as a pedometer and grow into more multipurpose devices such as a
smartwatch, which will consist of several sensors which can monitor
several components within its area of use.

The wearable devices are implementing tness and health monitoring by
using a mixture of sensors, such as motion, pulse, hydration and skin
temperature sensors. All of these wearables will therefore generate a
massive amount of data about the person who are using them. This data
can be considered as highly sensitive information, as it can unveil a
lot about their user's health, and the manufacturers of these devices
knows this. Dana Liebelson, a reporter for Hungton Post, queried several
US-based tness device companies about their privacy. One of the replies
she got, was that "the company does not sell information collected from
the device that can identify individual users", but that they were
considering marketing aggregate information that cannot be linked back
to an individual. As we saw in section 2.2 and 2.3 , many of the popular
methods for aggregating and anonymizing a dataset carries an inherent
risk of a privacy breach.

A study by Raij et al? have been performed to try to measure the privacy
concerns of people using wearable sensors, and found that activity
trackers that monitor heart rate, steps, and pulse for instance, was
usually seen as inoensive to the users’ privacy at the start of the
study. The researchers then had some of the testers wear sensors, and
could from the resulting data infer periods of heightened stress, as
well as derive certain context and behaviors that could trigger the
aforementioned stress. The participants were then given a similar
questionnaire to the initial one, and many then reported a heightened
sense of concern.

This is where we see a potential application for our distributed
framework. Although there would be some initial problems due to our
learner at the moment requiring labeled data to create a classier,
there can be potential in an health application where users keep control
of their own data. A potential solution could be to employ an
unsupervised learning technique based on clustering. Research have been
performed for testing the validity of applying dierential privacy to
clustering techniques and yielded positive results. Combining this with
the results of

Find the health paper from last year that did mini-clustering online

Find the health paper from last year that did mini-clustering online

-Future homes can potentially track you through you phone, or similar
device.

-You don't want to send this data to someone else, but what you can
learn from the data can be highly useful in your daily life if analyzed.

Kevin Fong and the England rugby team. Monitor heart rate, step balance,
and a lot of other factors. Can pick up injuries and illnesses well
before any doctor. This will trickle

41

Chapter 6. Analysis

down into daily use over the next decade, and let people potentially
discover illnesses before they even occur.

6.4.2 Private sharing of business data

A potentially interesting and lucrative market can be found in
facilitating the sharing of data between businesses in a private manner.
Our motivating example is found in the business of oil market analysis,
where competing rms gather a lot of data about oil price, rig
placements, supply ship availability, and more. They use this data to
create analytical models which help them in their work, and they also
sell this information to external clients. Often the rms would like to
collaborate their models or their data with their competitors. This
could be to validate that they are seeing the same trends or any other
reason, but due to the sensitive nature of their data and their fear of
losing a competitive edge, they cannot do this in a practical way.

It is in a situation like this that our distributed learner could be
applied, and allow the sharing of data between competitors as our system
could provide a privacy guarantee to all of the participants. The
participants would never lose control of their data, as all they would
need to install a program that allows them to connect as a peer in our
network: No third party would ever need access to their data.

42

Chapter 7

Conclusion

7.1 Threats To Validity

7.1.1 Platform

A potential threat to the validity of our conclusion/work , is how we
performed the setup

Fix this sentence

Fix this sentence

of the Jade platform. Since we wanted to perform our experiments on over
a range of parameters, we needed to nd a way to reset the platform
after a successive experiment and re-run it with a new set of
congurations. We solved this by having a jade agent called
CompletionAgent be responsible for waiting for every peer to message
indicating their completion, which would trigger the CompletionAgent to
deregister all the peers from the MainContainer and then reset the whole
environment. The environment would then be set up again with new
parameters.

What we see as a potential source for concern in this process is the
possibility for error during the deregistration. During the
implementation of this process we encountered some problems in making it
work, as the CompletionAgent seemed to take an unreasonable amount of
time in completing its purpose. Although we found a solution to this
problems, there is still a risk that peers do not deregister as they
should and carry through into the next iteration of testing. This could
lead to false information being injected into our experiment, which
would skew our results.

We have however minimized this risk by continuously developing unit test
to verify new code additions, as well as using JADE's native GUI to
supervise the behavior of the peers while running. We therefore conclude
that the risk is negligible.

7.1.2 Resource Consumption

A clear weakness of our system is our lack of formal analysis of
resource consumption and scalability. In

What have we done to to guarantee scalability? Is it enough to say it
should scale well?

43

Chapter 7. Conclusion

Due to time constraints, we've had to take certain shortcuts while
implementing our systems. The most glaring liability for system
scalability is our use of a single GroupFormingManager to handle
allocating aggregation groups for the participating peers. This manager
would quickly become a bottleneck in our system if we wanted to scale
the amount of peers beyond just a small mass of users. Our solution to
this predicament is found in section 7.2 , where we provide a solution
in the form of the Newscast algorithm.

7.1.3 Homomorphic encryption

As mentioned in Section 3.6 , homomorphic encryption is still in an
infantile stage of development and therefore cannot be called a
well-proven technology. Our method for aggregating models from various
peers is based on a homomorphic encryption scheme developed by Pathak et
al. but due to time constraints we could not actually implement it and
instead had to opt for simulating the results of applying this scheme.
We therefore do not have real-world results that can validate the
applicability of this scheme, nor do we know if applying this scheme
would lead to increased run-time and resource consumption. While this
remains an interesting area for future research, as it stands now it
remains a possible threat to the validity of the results we've achieved
and therefore also the conclusion we have drawn from them.

7.2 Future Work

Further develop and test the propagation of aggregated models. We
experienced that when we shared the aggregated models globally in our
network, we could decrease the SD in our classication error, as well as
sometimes improving the classier. Further research should go in
expanding this behavior, as you could potentially propagate models only
to peers in geographic and/or demographic vicinity. This could possibly
lead to more specialized models, which could give better classication
rate to a specialized subset of peers.

Another important area of research would be to further test the
applicability of peers sharing data to create better aggregated models.
Our original research questions was designed to explore the validity of
our proposed method of doing dierentially private machine learning, and
our current research has been limited to testing on a small amount of
datasets which is publicly available. In the future more research is
needed on datasets with

text

text

an uneven underlying distribution, which could potentially provide
results highlighting the usefulness of sharing information between
peers. An ideal dataset would be one where each peer only holds data
which makes up only a part of the solution.

This last sentence is crap and I hate it

This last sentence is crap and I hate it

Implement the Newscast algorithm for selecting peers. The Newscast
algorithm is a gossip protocol which facilitates a robust spread of
information. The core of the protocol involves periodic and pairvise
interaction between processes. Implementing this algorithm would allow
our system to scale better when a big number of peers are added to the
network. The biggest bottleneck of our system at the moment is the peer
sampling during the group forming, as it requires a single agent to act
as a manager for how groups are formed. The basic idea of the Newscast
algorithm is that each node, or peer in our situation, has a partial
view of the system. All nodes exchange their views periodically, which
allows

44

7.2 Future Work

them to keep an up-to-date view locally and spread their information
throughout the network. Further research into this algorithm would allow
us to customize this algorithm so that peers in our network could form
groups based on their partial views of the network.

Full data protection for each peer's data. This would involve dividing
the epsilon by the biggest dataset size, as formalized by Dwork in .
This is an ever tighter privacy guarantee,

citation

citation

but it would potentially mean that the results would contain too much
noise. To test this we would need a massive dataset, as we would need to
test the correlation between dataset size, and amount of noise added to
each peer. (More noise needs more data to smooth out.)

Real world case which takes humans into account. Right now research in
privacy is all about the technical details, and try to get it as close
as possible to existing methods. Without some kind of popular support,
the method will never see practice in real-world applications.

Work on a system that would work in a online setting. It could
potentially improve the system, as you would have new data coming in
which could replace old data with spent budgets, but it would also be
potentially a big tradeo as you won't have the same data history as you
would have in a system without dierential privacy. Dwork has written
about this in her book, so we can take inspiration from there. (As well
as our own paper)

Security mechanisms for stopping sabotage. In our current system we have
assumed that the peers will be honest-but-curious when sharing their
data, meaning that we have no way of detecting dishonest peers. In a
real world system there would need to be safeguards against people which
intend to either destroy the validity of the classier created by
feeding misinformation into the system, or people who tries to intercept
and expose the data from other peers. Potential research ares would be
intrusion detection in distributed systems, fraud detection, trust
networks and reputation systems, and further research into encryption.

45

Chapter 7. Conclusion

46

Notes

o Right word? 2

Figure: Picture showing the laplacian distribution 11

o Not sure if we need the pic. Alex: I think we do. 11

o add a section explaining that while it would seem that sensitivity is
would often be high, some functions like count are naturally low
sensitivity, and that steps such as clamping can be taken to reduce
sensitivity 11

o add term between rst and second 14

o Prove or remove. 14

o Show/explain that the same holds for the lower bound -epsilon also, as
in Pathak 14

o Add proof of above sensitivity 14

o Explain how we can protect whole user sets and the balance between
data set sizes and limits 14

o Consider remaking this gure so we don't have to cite it 16

o Write how we adapted the use of JADE here 16

o Do we need to talk about a method of HE that doesn't really work for
our requirements? 17

o Give analysis of time and space complexities and how much this taxes a
device. 17

o is this correct reference formatting? 20

o Rationalize why we have included 1 in the 10-inteval experiments - it
is because 1 is a very interesting edge case. Should also talk about the
signicance of group size 1 in analysis. 26

o Explain the specic way of how regularization was establish. Probably:
whenever number of peers changed, we ran an experiment over
regularization with other parameters xed to constant values. Explain
why this is okay. We should also discuss possible problems with this. 26

o Explain why we are doing the inital test with a range of
regularization. Essentially, it is because epsilon can be seen as xed,
learning rate is found by CV and lambda is the only remaining parameter
that is essentialy to tuning the performance of individual models.] 26

47

Chapter 7. Conclusion

o Talk about the problem of tuning regularization. We are essentially
doing a global selection of regularization. This could be dicult. Better
to communicate the minimum, perhaps? But then, which regularization
should peers pick locally? The highest one that stil has good
performance? 26

o Insert logistic regression training algorithm including hyperparameter
usage 26

o insert experiment here 27

o Defend randomSample procedure using Newscast reference 28

o Denitely talk more about the bagging eect, either here or in the
analysis section 29

o This section should maybe go somewhere else, but where? 29

Figure: Figure explaining our framework 29

o Model training on agent instantiation. 30

o Make a table with baseline classication rate for logistic regression,
put in our values. 1 Peer with no noise, 1 peer with noise, best case
from other experiments that we want to highlight. This can lead to a
discussion of why our results are not as good, where they show promise
etc 33

o Fix this last sentence. It is not that good 35

o Talk about how it is interesting that regularization now must respect
something other than just model performance, and oer some guidelines on
how regularization should be considered, especially in a setting like
ours, with many independent parties 37

o Rewrite this if we don't use phases 37

o Add a reection talking about why this makes sense. 37

o Figure out if there is some way we could test or prove that this is
not the case. 39

o look at a couple of vectors trained with large subsets of data and see
if they are similar. 39

o See if we have time to do this. We REALLY sho1uld, otherwise it will
very possibly give a lower grade. 39

o redo this experiment above with regularization=0, as indicated by the
previous gures shown 39

o Explain what cold start issues is. Discuss how this is very prominent
in our sitaution, and the dierent ways we can deal with the cold start,
and pros and cons of those approaches. 39

o Find the health paper from last year that did mini-clustering online
41

o Fix this sentence 43

o What have we done to to guarantee scalability? Is it enough to say it
should scale well? 43

o text 44

o This last sentence is crap and I hate it 44

o citation 45

48

Appendix

Write your appendix here...

49
